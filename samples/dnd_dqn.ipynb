{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyyaml\n",
    "# !pip install dndice\n",
    "# !pip install python-i18n\n",
    "# !pip install gymnasium\n",
    "# !pip install inflect\n",
    "# !pip install collections-extended\n",
    "# !pip install openai\n",
    "# !pip install -e ..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "from natural20.map import Map, Terrain\n",
    "from natural20.battle import Battle\n",
    "from natural20.player_character import PlayerCharacter\n",
    "from natural20.map_renderer import MapRenderer\n",
    "from natural20.die_roll import DieRoll\n",
    "from natural20.generic_controller import GenericController\n",
    "from natural20.utils.utils import Session\n",
    "from natural20.actions.move_action import MoveAction\n",
    "from natural20.action import Action\n",
    "from natural20.gym.dndenv import dndenv\n",
    "from gymnasium import register, envs, make\n",
    "from model import QNetwork\n",
    "import torch\n",
    "import tqdm as tqdm\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import gc\n",
    "import numpy as np\n",
    "import sys\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make(\"dndenv-v0\", root_path=\"../templates\")\n",
    "print(env.observation_space)\n",
    "print(env.action_space.sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = QNetwork(device=device)\n",
    "model.to(device)\n",
    "state, info = env.reset()\n",
    "moves = info[\"available_moves\"]\n",
    "\n",
    "model.eval()\n",
    "print(model(state, moves[0]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trajectory(env, model, policy='e-greedy', temperature=5.0, epsilon=0.1, quick_exit=False):\n",
    "    state, info = env.reset()\n",
    "    done = False\n",
    "    truncated = False\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    dones = []\n",
    "    truncateds = []\n",
    "    infos = []\n",
    "\n",
    "    while not done and not truncated:\n",
    "        # instead of sampling  (e.g. env.action_space.sample()) we can ask help from the enivronment to obtain valid moves\n",
    "        # as there are sparse valid moves in the environment\n",
    "        available_moves = info[\"available_moves\"]\n",
    "        with torch.no_grad():\n",
    "            if policy == 'e-greedy':\n",
    "                if random.random() < epsilon:\n",
    "                    chosen_index = random.choice(range(len(available_moves)))\n",
    "                else:\n",
    "                    values = torch.stack([model(state, move) for move in available_moves])\n",
    "                    chosen_index = torch.argmax(values).item()\n",
    "            elif policy == 'greedy':\n",
    "                    values = torch.stack([model(state, move) for move in available_moves])\n",
    "                    chosen_index = torch.argmax(values).item()\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown policy: {policy}\")\n",
    "        \n",
    "        action = available_moves[chosen_index]\n",
    "        state, reward, done, truncated, info = env.step(action)       \n",
    "        \n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "        truncateds.append(truncated)\n",
    "        infos.append(info)\n",
    "\n",
    "        if done or truncated:\n",
    "            break    \n",
    "        \n",
    "    return states, actions, rewards, dones, truncateds, infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory = generate_trajectory(env, model)\n",
    "print(trajectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 10\n",
    "avg_reward = 0\n",
    "for i in tqdm.tqdm(range(EPISODES)):\n",
    "    states, actions, rewards, dones, truncateds, infos = generate_trajectory(env, model)\n",
    "    avg_reward += sum(rewards)\n",
    "\n",
    "avg_reward /= EPISODES\n",
    "print(f\"Average reward: {avg_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, states, actions, rewards, is_terminal):\n",
    "        self.buffer.append((states, actions, rewards, is_terminal))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        buffer = list(self.buffer)\n",
    "        indices = np.random.choice(len(buffer), batch_size)\n",
    "        states, actions, rewards, is_terminals = zip(*[buffer[idx] for idx in indices])\n",
    "        return states, actions, rewards, is_terminals\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    # memory usage of the buffer in bytes\n",
    "    def memory_usage(self):\n",
    "        total_size = 0\n",
    "        for item in self.buffer:\n",
    "            states, actions, rewards, is_terminals = item\n",
    "            for s in states:\n",
    "                total_size += sys.getsizeof(s)\n",
    "            total_size += sys.getsizeof(actions)\n",
    "            total_size += sys.getsizeof(rewards)\n",
    "            total_size += sys.getsizeof(is_terminals)\n",
    "\n",
    "        return total_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# generate a batch of trajectories and store them in the replay buffer\n",
    "def generate_batch_trajectories(env, model, n_rollout, replay_buffer: ReplayBuffer, temperature=5.0, epsilon=0.1, policy='e-greedy'):\n",
    "    print(f\"generating {n_rollout} rollouts\")\n",
    "    for _ in range(n_rollout):\n",
    "        state, action, reward, done, truncated, info = generate_trajectory(env, model, temperature=temperature, epsilon=epsilon, policy=policy)\n",
    "        replay_buffer.push(state, action, reward, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_STEPS = 10\n",
    "TRAJECTORY_POLICY = \"e-greedy\"\n",
    "NUM_UPDATES = 1\n",
    "TEMP_DECAY = 0.999\n",
    "BUFFER_CAPACITY = 2000\n",
    "FRAMES_TO_STORE = 2\n",
    "MAX_STEPS = 1000\n",
    "BATCH_SIZE = 32\n",
    "TARGET_UPDATE_FREQ = 1\n",
    "T_HORIZON = 1024\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_FINAL = 0.02\n",
    "EPSILON_DECAY_FRAMES = 10**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make(\"dndenv-v0\", root_path=\"../templates\")\n",
    "\n",
    "def train(env, gamma, learning_rate, max_steps=MAX_STEPS, n_rollout=8, seed=1337):\n",
    "  print(f\"training with gamma {gamma} and learning rate {learning_rate}\")\n",
    "  env.seed(seed)\n",
    "\n",
    "  replay_buffer = ReplayBuffer(100)\n",
    "\n",
    "  # load model checkpoint if available\n",
    "  model = QNetwork(device).to(device)\n",
    "  target_model = QNetwork(device).to(device)\n",
    "\n",
    "  # intialize target network with the same weights as the model\n",
    "  target_model.load_state_dict(model.state_dict())\n",
    "\n",
    "  optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "  temperature = 5.0\n",
    "  reward_per_episode = []\n",
    "  epsilon = EPSILON_START\n",
    "\n",
    "  for step in range(max_steps):\n",
    "    if TRAJECTORY_POLICY == 'softmax':\n",
    "      print(f\"step {step} t={temperature}\")\n",
    "    elif TRAJECTORY_POLICY == 'e-greedy':\n",
    "      print(f\"step {step} epsilon={epsilon}\")\n",
    "    else:\n",
    "      print(f\"step {step}\")\n",
    "    generate_batch_trajectories(env, model, n_rollout, replay_buffer, temperature=temperature, epsilon=epsilon, policy=TRAJECTORY_POLICY)\n",
    "\n",
    "    states, actions, rewards, is_terminals = replay_buffer.sample(BATCH_SIZE)\n",
    "    rewards_collected = 0\n",
    "    for _ in range(NUM_UPDATES):\n",
    "      rewards_collected = 0\n",
    "      total_loss = 0.0\n",
    "      \n",
    "      for i in range(len(states)):\n",
    "        s = states[i]\n",
    "        a = actions[i]\n",
    "        r = torch.tensor(rewards[i]).to(device)\n",
    "        is_terminal = torch.tensor(is_terminals[i]).to(device)\n",
    "        \n",
    "        q_target = target_model(s, a).detach()\n",
    "        \n",
    "        targets = r + gamma * q_target * is_terminal\n",
    "        output = model(s, a)\n",
    "        q_sa = output\n",
    "\n",
    "        # print(f\"q_sa {q_sa.shape}\")\n",
    "        # print(f\"targets {targets.shape}\")\n",
    "        value_loss = nn.MSELoss()(q_sa, targets)\n",
    "        optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        total_loss += value_loss.item()\n",
    "        rewards_collected += r.sum().item()\n",
    "        optimizer.step()\n",
    "      print(f\"total loss {total_loss/len(states)}\")\n",
    "\n",
    "    # save model checkpoint\n",
    "\n",
    "    if step % 10 == 0:\n",
    "      # torch.save(model.state_dict(), f\"model_{step}.pt\")\n",
    "      torch.save(model.state_dict(), f\"model_{gamma}_{lr}.pt\")\n",
    "\n",
    "    if step % 100 == 0:\n",
    "      torch.save(model.state_dict(), f\"model_{gamma}_{lr}_{step}.pt\")\n",
    "\n",
    "    # if step % 5 == 0 and step > 0:\n",
    "    _, _, rewards, _, _, _ = generate_trajectory(env, model, policy='greedy')\n",
    "    total_reward = sum(rewards)\n",
    "    \n",
    "    reward_per_episode.append(total_reward)\n",
    "\n",
    "    print(f\"total reward: {total_reward}\")\n",
    "    print(f\"{step}: rewards {rewards_collected}\")\n",
    "    gc.collect()\n",
    "    \n",
    "    # decay temp\n",
    "    temperature = np.max([0.1, temperature * TEMP_DECAY])\n",
    "\n",
    "    # decay epsilon\n",
    "    epsilon = EPSILON_FINAL + (EPSILON_START - EPSILON_FINAL) * np.exp(-1.0 * step / EPSILON_DECAY_FRAMES)\n",
    "\n",
    "    if step % TARGET_UPDATE_FREQ == 0:\n",
    "      # calculate the avg change weights of the model with the target model\n",
    "      total_change = 0\n",
    "      for p, p_target in zip(model.parameters(), target_model.parameters()):\n",
    "        total_change += torch.abs(p - p_target).sum().item()\n",
    "      print(f\"total change: {total_change}\")\n",
    "\n",
    "      target_model.load_state_dict(model.state_dict())\n",
    "\n",
    "  env.close()\n",
    "  return reward_per_episode\n",
    "\n",
    "seed = 1337\n",
    "# Create a grid of learning rates and gammas\n",
    "learning_rates = [0.0001, 0.001]\n",
    "gammas = [0.99, 0.1]\n",
    "\n",
    "results = {}\n",
    "for lr_index, lr in enumerate(learning_rates):\n",
    "  results[lr] = {}\n",
    "  for g_index, gamma in enumerate(gammas):\n",
    "    reward_per_episode = train(env, gamma, lr, max_steps=MAX_STEPS, seed=seed*lr_index + g_index)\n",
    "    results[learning_rates.index(lr), gammas.index(gamma)] = reward_per_episode\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai221",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
