{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyyaml\n",
    "!pip install dndice\n",
    "!pip install python-i18n\n",
    "!pip install gymnasium\n",
    "!pip install inflect\n",
    "!pip install collections-extended\n",
    "!pip install openai\n",
    "!pip install -e ..\n",
    "!pip install ipywidgets\n",
    "!pip install iprogress\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium import make\n",
    "from model import QNetwork\n",
    "from natural20.gym.dndenv import dndenv\n",
    "import torch\n",
    "import tqdm as tqdm\n",
    "import tqdm.notebook as tqdm\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import gc\n",
    "import numpy as np\n",
    "import sys\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make(\"dndenv-v0\", root_path=\"map_with_obstacles\", show_logs=True)\n",
    "print(env.observation_space)\n",
    "print(env.action_space.sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQN Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAJECTORY_POLICY = \"e-greedy\"\n",
    "NUM_UPDATES = 2\n",
    "TEMP_DECAY = 0.999\n",
    "BUFFER_CAPACITY = 200\n",
    "FRAMES_TO_STORE = 2\n",
    "MAX_STEPS = 1000\n",
    "BATCH_SIZE = 32\n",
    "TARGET_UPDATE_FREQ = 1\n",
    "T_HORIZON = 1024\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_FINAL = 0.02\n",
    "EPSILON_DECAY_FRAMES = 10**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = QNetwork(device=device)\n",
    "model.to(device)\n",
    "state, info = env.reset()\n",
    "moves = info[\"available_moves\"]\n",
    "\n",
    "model.eval()\n",
    "print(model(state, moves[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trajectory(env, model, policy='e-greedy', temperature=5.0, epsilon=0.1, horizon=30, quick_exit=False):\n",
    "    state, info = env.reset()\n",
    "    done = False\n",
    "    truncated = False\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    dones = []\n",
    "    truncateds = []\n",
    "    infos = []\n",
    "    truncated = False\n",
    "    for _ in range(horizon):\n",
    "        # instead of sampling  (e.g. env.action_space.sample()) we can ask help from the enivronment to obtain valid moves\n",
    "        # as there are sparse valid moves in the environment\n",
    "        available_moves = info[\"available_moves\"]\n",
    "        with torch.no_grad():\n",
    "            if policy == 'boltzmann':\n",
    "                values = torch.stack([model(state, move).squeeze() for move in available_moves])\n",
    "                if len(values) > 1:\n",
    "                    values = values / temperature\n",
    "                    values = torch.exp(values)\n",
    "                    values = values / torch.sum(values)\n",
    "                    chosen_index = torch.multinomial(values, 1).item()\n",
    "                else:\n",
    "                    chosen_index = 0\n",
    "            elif policy == 'e-greedy':\n",
    "                if random.random() < epsilon:\n",
    "                    chosen_index = random.choice(range(len(available_moves)))\n",
    "                else:\n",
    "                    values = torch.stack([model(state, move) for move in available_moves])\n",
    "                    chosen_index = torch.argmax(values).item()\n",
    "            elif policy == 'greedy':\n",
    "                    values = torch.stack([model(state, move) for move in available_moves])\n",
    "                    chosen_index = torch.argmax(values).item()\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown policy: {policy}\")\n",
    "        \n",
    "        action = available_moves[chosen_index]\n",
    "        state, reward, done, truncated, info = env.step(action)       \n",
    "        \n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "        truncateds.append(truncated)\n",
    "        infos.append(info)\n",
    "\n",
    "        if done:\n",
    "            break    \n",
    "        if truncated:\n",
    "            truncated = True\n",
    "            break\n",
    "    \n",
    "    return states, actions, rewards, dones, truncateds, infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory = generate_trajectory(env, model)\n",
    "print(trajectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 10\n",
    "avg_reward = 0\n",
    "for i in tqdm.tqdm(range(EPISODES)):\n",
    "    states, actions, rewards, dones, truncateds, infos = generate_trajectory(env, model)\n",
    "    avg_reward += sum(rewards)\n",
    "\n",
    "avg_reward /= EPISODES\n",
    "print(f\"Average reward: {avg_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, states, actions, rewards, infos, is_terminal):\n",
    "        self.buffer.append((states, actions, rewards, infos, is_terminal))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.choice(len(self.buffer), batch_size)\n",
    "        states, actions, rewards, infos, is_terminals = zip(*[self.buffer[idx] for idx in indices])\n",
    "        return states, actions, rewards, infos, is_terminals\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    # memory usage of the buffer in bytes\n",
    "    def memory_usage(self):\n",
    "        total_size = 0\n",
    "        for item in self.buffer:\n",
    "            states, actions, rewards, infos, is_terminals = item\n",
    "            for s in states:\n",
    "                total_size += sys.getsizeof(s)\n",
    "            total_size += sys.getsizeof(actions)\n",
    "            total_size += sys.getsizeof(rewards)\n",
    "            total_size += sys.getsizeof(infos)\n",
    "            total_size += sys.getsizeof(is_terminals)\n",
    "\n",
    "        return total_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a batch of trajectories and store them in the replay buffer\n",
    "def generate_batch_trajectories(env, model, n_rollout, replay_buffer: ReplayBuffer, temperature=5.0, epsilon=0.1, horizon=30, policy='e-greedy'):\n",
    "    print(f\"generating {n_rollout} rollouts\")\n",
    "    for _ in range(n_rollout):\n",
    "        state, action, reward, done, truncated, info = generate_trajectory(env, model, temperature=temperature,\n",
    "                                                                           epsilon=epsilon,\n",
    "                                                                           horizon=horizon,policy=policy)\n",
    "        replay_buffer.push(state, action, reward, info, done or truncated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, gamma, learning_rate, max_steps=MAX_STEPS, use_td_target=True, trajectory_policy='e-greedy', n_rollout=8, seed=1337):\n",
    "  print(f\"training with gamma {gamma} and learning rate {learning_rate}\")\n",
    "  env.seed(seed)\n",
    "\n",
    "  replay_buffer = ReplayBuffer(BUFFER_CAPACITY)\n",
    "  # load model checkpoint if available\n",
    "  model = QNetwork(device).to(device)\n",
    "  target_model = QNetwork(device).to(device)\n",
    "\n",
    "  # intialize target network with the same weights as the model\n",
    "  target_model.load_state_dict(model.state_dict())\n",
    "\n",
    "  optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "  temperature = 5.0\n",
    "  reward_per_episode = []\n",
    "  epsilon = EPSILON_START\n",
    "\n",
    "  for step in tqdm.tqdm(range(max_steps)):\n",
    "    if trajectory_policy == 'boltzmann':\n",
    "      print(f\"step {step} t={temperature}\")\n",
    "    if trajectory_policy == 'softmax':\n",
    "      print(f\"step {step} t={temperature}\")\n",
    "    elif trajectory_policy == 'e-greedy':\n",
    "      print(f\"step {step} epsilon={epsilon}\")\n",
    "    else:\n",
    "      print(f\"step {step}\")\n",
    "    generate_batch_trajectories(env, model, n_rollout, replay_buffer, temperature=temperature,\n",
    "                                epsilon=epsilon, policy=trajectory_policy, horizon=T_HORIZON)\n",
    "\n",
    "    states, actions, rewards, infos, is_terminals = replay_buffer.sample(BATCH_SIZE)\n",
    "    rewards_collected = 0\n",
    "    for _ in range(NUM_UPDATES):\n",
    "      rewards_collected = 0\n",
    "      total_loss = 0.0\n",
    "      \n",
    "      for i in range(len(states)):\n",
    "        s = states[i]\n",
    "        a = actions[i]\n",
    "        env_info = infos[i]\n",
    "        r = torch.tensor(rewards[i]).to(device).unsqueeze(1)\n",
    "        is_terminal = torch.tensor(is_terminals[i]).to(device).unsqueeze(1)\n",
    "        \n",
    "        if use_td_target:\n",
    "          with torch.no_grad():\n",
    "            q_targets = target_model(s, a).detach()\n",
    "        else: # Q-learning target == \"slow\"\n",
    "          q_targets = torch.zeros(len(env_info)).to(device)\n",
    "          for index, (state, info) in enumerate(zip(s, env_info)):\n",
    "            q_values = [target_model([state], [avail_action]).detach() for avail_action in info[\"available_moves\"]]\n",
    "            if len(q_values) == 0:\n",
    "              q_targets[index] = 0\n",
    "            else:\n",
    "              q_targets[index] = torch.max(torch.stack(q_values)).item()\n",
    "        targets = r + gamma * q_targets * is_terminal\n",
    "        output = model(s, a)\n",
    "        q_sa = output\n",
    "\n",
    "        value_loss = nn.MSELoss()(q_sa, targets)\n",
    "        optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        total_loss += value_loss.item()\n",
    "        rewards_collected += r.sum().item()\n",
    "        optimizer.step()\n",
    "      print(f\"total loss {total_loss/len(states)}\")\n",
    "\n",
    "    # save model checkpoint\n",
    "\n",
    "    if step % 10 == 0:\n",
    "      # torch.save(model.state_dict(), f\"model_{step}.pt\")\n",
    "      torch.save(model.state_dict(), f\"model_{gamma}_{learning_rate}.pt\")\n",
    "\n",
    "    if step % 100 == 0:\n",
    "      torch.save(model.state_dict(), f\"model_{gamma}_{learning_rate}_{step}.pt\")\n",
    "\n",
    "    eval_rewards = []\n",
    "    for _ in range(5):\n",
    "      _, _, rewards, _, _, _ = generate_trajectory(env, model, policy='greedy')\n",
    "      total_reward = sum(rewards)\n",
    "      eval_rewards.append(total_reward)\n",
    "      \n",
    "    avg_rewards = np.mean(eval_rewards)\n",
    "    print(f\"eval rewards: {avg_rewards}\")\n",
    "    reward_per_episode.append(avg_rewards)\n",
    "\n",
    "    print(f\"total reward: {total_reward}\")\n",
    "    print(f\"{step}: avg rewards {avg_rewards}\")\n",
    "    gc.collect()\n",
    "    \n",
    "    # decay temp\n",
    "    temperature = np.max([0.1, temperature * TEMP_DECAY])\n",
    "\n",
    "    # decay epsilon\n",
    "    epsilon = EPSILON_FINAL + (EPSILON_START - EPSILON_FINAL) * np.exp(-1.0 * step / EPSILON_DECAY_FRAMES)\n",
    "\n",
    "    if step % TARGET_UPDATE_FREQ == 0:\n",
    "      # calculate the avg change weights of the model with the target model\n",
    "      total_change = 0\n",
    "      for p, p_target in zip(model.parameters(), target_model.parameters()):\n",
    "        total_change += torch.abs(p - p_target).sum().item()\n",
    "      print(f\"total change: {total_change}\")\n",
    "\n",
    "      target_model.load_state_dict(model.state_dict())\n",
    "\n",
    "  env.close()\n",
    "  return reward_per_episode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make(\"dndenv-v0\", root_path=\"map_with_obstacles\", show_logs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gomerin is unconscious.\n",
      "eval rewards: -6.0\n",
      "total reward: -10\n",
      "634: avg rewards -6.0\n",
      "total change: 355.10738998185843\n",
      "step 635 epsilon=0.5398563750847354\n",
      "generating 8 rollouts\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "seed = 1337\n",
    "# Create a grid of learning rates and gammas\n",
    "learning_rates = [0.001]\n",
    "gammas = [0.99]\n",
    "\n",
    "results = {}\n",
    "for lr in learning_rates:\n",
    "  results[lr] = {}\n",
    "  for gamma in gammas:\n",
    "    seed = seed + 1\n",
    "    reward_per_episode = train(env, gamma, lr, max_steps=MAX_STEPS, seed=seed)\n",
    "    results[lr][gamma] = reward_per_episode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize rewards per episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in results:\n",
    "  for gamma in results[item]:\n",
    "    print(f\"lr: {item} gamma: {gamma} rewards: {results[item][gamma]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for gamma in gammas:\n",
    "        plt.plot(results[lr][gamma], label=f\"lr={lr}, gamma={gamma}\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1337\n",
    "# Create a grid of learning rates and gammas\n",
    "learning_rates = [0.001]\n",
    "gammas = [0.99]\n",
    "\n",
    "results = {}\n",
    "for lr in learning_rates:\n",
    "  results[lr] = {}\n",
    "  for gamma in gammas:\n",
    "    seed = seed + 1\n",
    "    reward_per_episode = train(env, gamma, lr, max_steps=MAX_STEPS, seed=seed, trajectory_policy='boltzmann')\n",
    "    results[lr][gamma] = reward_per_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in results:\n",
    "  for gamma in results[item]:\n",
    "    print(f\"lr: {item} gamma: {gamma} rewards: {results[item][gamma]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for gamma in gammas:\n",
    "        plt.plot(results[lr][gamma], label=f\"lr={lr}, gamma={gamma}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
