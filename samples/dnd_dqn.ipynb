{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyyaml\n",
    "!pip install dndice\n",
    "!pip install python-i18n\n",
    "!pip install gymnasium\n",
    "!pip install inflect\n",
    "!pip install collections-extended\n",
    "!pip install openai\n",
    "!pip install -e ..\n",
    "!pip install ipywidgets\n",
    "!pip install iprogress\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium import make\n",
    "from model import QNetwork\n",
    "from natural20.gym.dndenv import dndenv\n",
    "from torchsummary import summary\n",
    "import torch\n",
    "import tqdm as tqdm\n",
    "import tqdm.notebook as tqdm\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import gc\n",
    "import numpy as np\n",
    "import sys\n",
    "import collections\n",
    "from natural20.utils.utils import Session\n",
    "from natural20.event_manager import EventManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config = \"map_with_obstacles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = Session(env_config, event_manager=EventManager())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show info about the environment and a render of the tabletop map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make(\"dndenv-v0\", root_path=\"map_with_obstacles\", show_logs=True,\n",
    "           custom_session=session,\n",
    "           damaged_based_reward=True,\n",
    "           render_mode=\"ansi\")\n",
    "env.reset()\n",
    "print(env.render())\n",
    "print(env.observation_space)\n",
    "print(env.action_space.sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQN Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAJECTORY_POLICY = \"e-greedy\"\n",
    "NUM_UPDATES = 2\n",
    "TEMP_DECAY = 0.999\n",
    "BUFFER_CAPACITY = 2000\n",
    "FRAMES_TO_STORE = 2\n",
    "MAX_STEPS = 2000\n",
    "BATCH_SIZE = 64\n",
    "TARGET_UPDATE_FREQ = 1\n",
    "T_HORIZON = 2048\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_FINAL = 0.02\n",
    "EPSILON_DECAY_FRAMES = 10**3\n",
    "EVAL_STEPS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = QNetwork(device=device)\n",
    "model.to(device)\n",
    "state, info = env.reset()\n",
    "moves = info[\"available_moves\"]\n",
    "model.eval()\n",
    "print(model(state, moves[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def act_with_policy(state, info, model, policy='e-greedy', temperature=5.0, epsilon=0.1):\n",
    "    available_moves = info[\"available_moves\"]\n",
    "    with torch.no_grad():\n",
    "        if policy == 'boltzmann':\n",
    "            values = torch.stack([model(state, move).squeeze() for move in available_moves])\n",
    "            if len(values) > 1:\n",
    "                if temperature != 0:\n",
    "                    values = values / temperature\n",
    "                else:\n",
    "                    raise ValueError(\"Temperature is zero, which can lead to division by zero.\")\n",
    "\n",
    "                # Stabilizing the exponential calculation\n",
    "                values = values - torch.max(values)  # Subtract the max value for numerical stability\n",
    "                values = torch.exp(values)\n",
    "                sum_values = torch.sum(values)\n",
    "\n",
    "                if sum_values > 0:\n",
    "                    values = values / sum_values\n",
    "                    chosen_index = torch.multinomial(values, 1).item()\n",
    "                else:\n",
    "                    print(\"Sum of exponentiated values is zero. Adjust the model or input.\")\n",
    "                    chosen_index = torch.randint(len(available_moves), (1,)).item()\n",
    "            else:\n",
    "                chosen_index = 0\n",
    "        elif policy == 'e-greedy':\n",
    "            if random.random() < epsilon:\n",
    "                chosen_index = random.choice(range(len(available_moves)))\n",
    "            else:\n",
    "                values = torch.stack([model(state, move) for move in available_moves])\n",
    "                chosen_index = torch.argmax(values).item()\n",
    "        elif policy == 'greedy':\n",
    "                values = torch.stack([model(state, move) for move in available_moves])\n",
    "                chosen_index = torch.argmax(values).item()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown policy: {policy}\")\n",
    "    \n",
    "    return available_moves[chosen_index]\n",
    "\n",
    "def generate_trajectory(env, model, policy='e-greedy', temperature=5.0, epsilon=0.1, horizon=2048, quick_exit=False):\n",
    "    state, info = env.reset()\n",
    "    done = False\n",
    "    truncated = False\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    dones = []\n",
    "    truncateds = []\n",
    "    infos = []\n",
    "    truncated = False\n",
    "    for _ in range(horizon):\n",
    "        # instead of sampling  (e.g. env.action_space.sample()) we can ask help from the enivronment to obtain valid moves\n",
    "        # as there are sparse valid moves in the environment\n",
    "        action = act_with_policy(state, info, model, policy, temperature, epsilon)\n",
    "        next_state, reward, done, truncated, next_info = env.step(action)       \n",
    "        \n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "        truncateds.append(truncated)\n",
    "        infos.append(info)\n",
    "\n",
    "        if done:\n",
    "            break    \n",
    "        if truncated:\n",
    "            truncated = True\n",
    "            break\n",
    "        state = next_state\n",
    "        info = next_info\n",
    "        \n",
    "    states.append(next_state)\n",
    "    infos.append(next_info)\n",
    "    actions.append((-1, (0,0), (0,0), 0))\n",
    "    return states, actions, rewards, dones, truncateds, infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory = generate_trajectory(env, model, epsilon=1.0)\n",
    "print(trajectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 10\n",
    "total_rewards = 0\n",
    "for i in tqdm.tqdm(range(EPISODES)):\n",
    "    states, actions, rewards, dones, truncateds, infos = generate_trajectory(env, model, epsilon=1.0)\n",
    "    total_rewards += sum(rewards)\n",
    "\n",
    "avg_reward = total_rewards/EPISODES\n",
    "print(f\"Average reward: {avg_reward} Total Reward: {total_rewards}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, states, actions, rewards, infos, is_terminal):\n",
    "        self.buffer.append((states, actions, rewards, infos, is_terminal))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.choice(len(self.buffer), batch_size)\n",
    "        states, actions, rewards, infos, is_terminals = zip(*[self.buffer[idx] for idx in indices])\n",
    "        return states, actions, rewards, infos, is_terminals\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    # memory usage of the buffer in bytes\n",
    "    def memory_usage(self):\n",
    "        total_size = 0\n",
    "        for item in self.buffer:\n",
    "            states, actions, rewards, infos, is_terminals = item\n",
    "            for s in states:\n",
    "                total_size += sys.getsizeof(s)\n",
    "            total_size += sys.getsizeof(actions)\n",
    "            total_size += sys.getsizeof(rewards)\n",
    "            total_size += sys.getsizeof(infos)\n",
    "            total_size += sys.getsizeof(is_terminals)\n",
    "\n",
    "        return total_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a batch of trajectories and store them in the replay buffer\n",
    "def generate_batch_trajectories(env, model, n_rollout, replay_buffer: ReplayBuffer, temperature=5.0, epsilon=0.1, horizon=30, policy='e-greedy'):\n",
    "    # print(f\"generating {n_rollout} rollouts\")\n",
    "    for _ in range(n_rollout):\n",
    "        state, action, reward, done, truncated, info = generate_trajectory(env, model, temperature=temperature,\n",
    "                                                                           epsilon=epsilon,\n",
    "                                                                           horizon=horizon,policy=policy)\n",
    "        replay_buffer.push(state, action, reward, info, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, gamma, learning_rate, max_steps=MAX_STEPS, use_td_target=True,\n",
    "          trajectory_policy='e-greedy',\n",
    "          label=\"dnd_egreedy\",\n",
    "          n_rollout=8,\n",
    "          seed=1337):\n",
    "  print(f\"training with gamma {gamma} and learning rate {learning_rate}\")\n",
    "  env.seed(seed)\n",
    "\n",
    "  replay_buffer = ReplayBuffer(BUFFER_CAPACITY)\n",
    "  # load model checkpoint if available\n",
    "  model = QNetwork(device).to(device)\n",
    "  target_model = QNetwork(device).to(device)\n",
    "\n",
    "  # intialize target network with the same weights as the model\n",
    "  target_model.load_state_dict(model.state_dict())\n",
    "\n",
    "  optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "  best_avg = -10\n",
    "  best_step = 0\n",
    "  temperature = 5.0\n",
    "  reward_per_episode = []\n",
    "  epsilon = EPSILON_START\n",
    "\n",
    "  for step in tqdm.tqdm(range(max_steps)):\n",
    "    # if trajectory_policy == 'boltzmann':\n",
    "    #   print(f\"step {step} t={temperature}\")\n",
    "    # if trajectory_policy == 'softmax':\n",
    "    #   print(f\"step {step} t={temperature}\")\n",
    "    # elif trajectory_policy == 'e-greedy':\n",
    "    #   print(f\"step {step} epsilon={epsilon}\")\n",
    "    # else:\n",
    "    #   print(f\"step {step}\")\n",
    "    generate_batch_trajectories(env, model, n_rollout, replay_buffer, temperature=temperature,\n",
    "                                epsilon=epsilon, policy=trajectory_policy, horizon=T_HORIZON)\n",
    "\n",
    "    states, actions, rewards, infos, is_terminals = replay_buffer.sample(BATCH_SIZE)\n",
    "    rewards_collected = 0\n",
    "    for _ in range(NUM_UPDATES):\n",
    "      rewards_collected = 0\n",
    "      total_loss = 0.0\n",
    "      \n",
    "      for i in range(len(states)):\n",
    "        s = states[i]\n",
    "        a = actions[i]\n",
    "        env_info = infos[i]\n",
    "        r = torch.tensor(rewards[i]).to(device).unsqueeze(1)\n",
    "        is_terminal = torch.tensor(is_terminals[i]).float().to(device).unsqueeze(1)\n",
    "        \n",
    "        if use_td_target:\n",
    "          with torch.no_grad():\n",
    "            s_next = s[1:]\n",
    "            a_next = a[1:]\n",
    "            q_targets = target_model(s_next, a_next).detach()\n",
    "        else: # Q-learning target == \"slow\"\n",
    "          with torch.no_grad():\n",
    "            s_next = s[1:]\n",
    "            s_info = env_info[1:]\n",
    "            q_targets = torch.zeros(len(s_next)).to(device)\n",
    "            \n",
    "            for index in range(len(s_info)):\n",
    "              info = s_info[index]\n",
    "              state = s_next[index]\n",
    "              \n",
    "              if len(state) == 0:\n",
    "                q_targets[index] = 0\n",
    "                continue\n",
    "              \n",
    "              total_available_moves = len(info[\"available_moves\"])\n",
    "              states_t = [state] * total_available_moves\n",
    "              avail_actions = info[\"available_moves\"]\n",
    "              assert len(states_t) > 0, \"No available states\"\n",
    "              assert len(avail_actions) > 0, \"No available moves\"\n",
    "              \n",
    "              q_values = target_model(states_t, avail_actions).detach().squeeze(1)\n",
    "              if len(q_values) == 0:\n",
    "                q_targets[index] = 0\n",
    "              else:\n",
    "                q_targets[index] = torch.max(q_values).item()\n",
    "\n",
    "            q_targets = q_targets.unsqueeze(1)\n",
    "            assert q_targets.shape == r.shape, f\"q_targets shape {q_targets.shape} != r shape {r.shape}\"\n",
    "\n",
    "        targets = r + gamma * q_targets * (1 - is_terminal)\n",
    "        \n",
    "        s_input = s[0:-1]\n",
    "        a_input = a[0:-1]\n",
    "        output = model(s_input, a_input)\n",
    "        q_sa = output\n",
    "\n",
    "        value_loss = nn.MSELoss()(q_sa, targets)\n",
    "        optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        total_loss += value_loss.item()\n",
    "        rewards_collected += r.sum().item()\n",
    "        optimizer.step()\n",
    "\n",
    "    # save model checkpoint\n",
    "\n",
    "    if step % 10 == 0:\n",
    "      # torch.save(model.state_dict(), f\"model_{step}.pt\")\n",
    "      eval_rewards = []\n",
    "      for _ in range(EVAL_STEPS):\n",
    "        _, _, rewards, _, _, _ = generate_trajectory(env, model, policy='greedy')\n",
    "        total_reward = sum(rewards)\n",
    "        eval_rewards.append(total_reward)\n",
    "        \n",
    "      avg_rewards = np.mean(eval_rewards)\n",
    "      # print(f\"eval rewards: {avg_rewards}\")\n",
    "      reward_per_episode.append(avg_rewards)\n",
    "\n",
    "      # print(f\"total reward: {total_reward}\")\n",
    "      print(f\"{step}: avg rewards {avg_rewards} best avg {best_avg}@{best_step}\")\n",
    "\n",
    "      if avg_rewards > best_avg:\n",
    "        print(f\"best: {avg_rewards}\")\n",
    "        best_avg = avg_rewards\n",
    "        best_step = step\n",
    "        torch.save(model.state_dict(), f\"model_best_{label}.pt\")\n",
    "\n",
    "      torch.save(model.state_dict(), f\"model_{label}_{gamma}_{learning_rate}.pt\")\n",
    "\n",
    "\n",
    "    if step % 100 == 0:\n",
    "      torch.save(model.state_dict(), f\"model_{label}_{gamma}_{learning_rate}_{step}.pt\")\n",
    "\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "    # decay temp\n",
    "    temperature = np.max([0.1, temperature * TEMP_DECAY])\n",
    "\n",
    "    # decay epsilon\n",
    "    epsilon = EPSILON_FINAL + (EPSILON_START - EPSILON_FINAL) * np.exp(-1.0 * step / EPSILON_DECAY_FRAMES)\n",
    "\n",
    "    if step % TARGET_UPDATE_FREQ == 0:\n",
    "      # calculate the avg change weights of the model with the target model\n",
    "      total_change = 0\n",
    "      for p, p_target in zip(model.parameters(), target_model.parameters()):\n",
    "        total_change += torch.abs(p - p_target).sum().item()\n",
    "      # print(f\"total change: {total_change}\")\n",
    "\n",
    "      target_model.load_state_dict(model.state_dict())\n",
    "\n",
    "  env.close()\n",
    "  return reward_per_episode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the location of the game configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_setup_path = \"map_with_obstacles\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the env setup. Note that we use damaged based rewards to give a denser reward signalling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(root_path):\n",
    "    return make(\"dndenv-v0\", root_path=root_path, show_logs=False, damage_based_reward=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(game_setup_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m gamma \u001b[38;5;129;01min\u001b[39;00m gammas:\n\u001b[1;32m     10\u001b[0m   seed \u001b[38;5;241m=\u001b[39m seed \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 11\u001b[0m   reward_per_episode \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_STEPS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_td_target\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m   results[lr][gamma] \u001b[38;5;241m=\u001b[39m reward_per_episode\n",
      "Cell \u001b[0;32mIn[14], line 92\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(env, gamma, learning_rate, max_steps, use_td_target, trajectory_policy, label, n_rollout, seed)\u001b[0m\n\u001b[1;32m     90\u001b[0m value_loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()(q_sa, targets)\n\u001b[1;32m     91\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 92\u001b[0m \u001b[43mvalue_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m value_loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     94\u001b[0m rewards_collected \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "seed = 1337\n",
    "# Create a grid of learning rates and gammas\n",
    "learning_rates = [0.0001]\n",
    "gammas = [0.99]\n",
    "\n",
    "results = {}\n",
    "for lr in learning_rates:\n",
    "  results[lr] = {}\n",
    "  for gamma in gammas:\n",
    "    seed = seed + 1\n",
    "    reward_per_episode = train(env, gamma, lr, max_steps=MAX_STEPS, seed=seed, use_td_target=False)\n",
    "    results[lr][gamma] = reward_per_episode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize rewards per episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in results:\n",
    "  for gamma in results[item]:\n",
    "    print(f\"lr: {item} gamma: {gamma} rewards: {results[item][gamma]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for gamma in gammas:\n",
    "        plt.plot(results[lr][gamma], label=f\"lr={lr}, gamma={gamma}\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(game_setup_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1337\n",
    "# Create a grid of learning rates and gammas\n",
    "learning_rates = [0.001]\n",
    "gammas = [0.99]\n",
    "\n",
    "results = {}\n",
    "for lr in learning_rates:\n",
    "  results[lr] = {}\n",
    "  for gamma in gammas:\n",
    "    seed = seed + 1\n",
    "    reward_per_episode = train(env, gamma, lr, max_steps=MAX_STEPS, seed=seed, trajectory_policy='boltzmann', label=\"boltzmann\")\n",
    "    results[lr][gamma] = reward_per_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in results:\n",
    "  for gamma in results[item]:\n",
    "    print(f\"lr: {item} gamma: {gamma} rewards: {results[item][gamma]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for gamma in gammas:\n",
    "        plt.plot(results[lr][gamma], label=f\"lr={lr}, gamma={gamma}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform some tests on the trained agent. Show a combat log from a fight against the rules based AI. Define a policy based on the Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "MAX_EPISODES = 500\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class ModelPolicy:\n",
    "    def __init__(self):\n",
    "        self.model = QNetwork(device=device)\n",
    "        self.model.to(device)\n",
    "        if not os.path.exists('model_best_blotzmann.pt'):\n",
    "            raise FileNotFoundError(\"Model file not found. Please run dnd_dqn.ipynb notebook to train an agent.\")\n",
    "        self.model.load_state_dict(torch.load('model_best_blotzmann.pt'))\n",
    "\n",
    "    def action(self, state, info):\n",
    "        available_moves = info[\"available_moves\"]\n",
    "        values = torch.stack([self.model(state, move) for move in available_moves])\n",
    "        for index, v in enumerate(values):\n",
    "            print(f\"{index}: {available_moves[index]} {v.item()}\")\n",
    "\n",
    "        chosen_index = torch.argmax(values).item()\n",
    "        return available_moves[chosen_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = make(\"dndenv-v0\", root_path=\"map_with_obstacles\", render_mode=\"ansi\", show_logs=True)\n",
    "\n",
    "observation, info = env.reset()\n",
    "\n",
    "print(\"=========================================\")\n",
    "print(\"Battle between an RL agent vs a Rules based AI\")\n",
    "print(\"=========================================\")\n",
    "model = ModelPolicy()\n",
    "action = action = model.action(observation, info)\n",
    "\n",
    "print(f\"selected action: {action}\")\n",
    "terminal = False\n",
    "episode = 0\n",
    "\n",
    "while not terminal and episode < MAX_EPISODES:\n",
    "    episode += 1\n",
    "    observation, reward, terminal, truncated, info = env.step(action)\n",
    "    print(env.render())\n",
    "    if not terminal and not truncated:\n",
    "        episode_name_with_padding = str(episode).zfill(3)\n",
    "\n",
    "        # display entity healths\n",
    "        print(f\"Turn {info['current_index']}\\n\")\n",
    "        print(f\"Reward: {reward}\\n\")\n",
    "        print(f\"health hero: {observation['health_pct']}\\n\")\n",
    "        print(f\"health enemy: {observation['health_enemy']}\\n\")\n",
    "        print(env.render())\n",
    "        \n",
    "        action = model.action(observation, info)\n",
    "        print(f\"agent selected action: {action}\")\n",
    "\n",
    "    if terminal or truncated:\n",
    "        print(f\"Reward: {reward}\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
