{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyyaml\n",
    "!pip install dndice\n",
    "!pip install python-i18n\n",
    "!pip install gymnasium\n",
    "!pip install inflect\n",
    "!pip install collections-extended\n",
    "!pip install openai\n",
    "!pip install -e ..\n",
    "!pip install ipywidgets\n",
    "!pip install iprogress\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium import make\n",
    "from model import QNetwork\n",
    "from natural20.gym.dndenv import dndenv\n",
    "import torch\n",
    "import tqdm as tqdm\n",
    "import tqdm.notebook as tqdm\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import gc\n",
    "import numpy as np\n",
    "import sys\n",
    "import collections\n",
    "from natural20.session import Session\n",
    "from natural20.event_manager import EventManager\n",
    "from natural20.gym.dqn.replay_buffer import ReplayBuffer\n",
    "import os\n",
    "from llm_interface import GPT4Interfacer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "  device = torch.device(\"mps\")\n",
    "else:\n",
    "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config = \"map_with_obstacles\"\n",
    "PROJECT_OUTPUT_PATH = \"model_weights_all\"\n",
    "if not os.path.exists(PROJECT_OUTPUT_PATH):\n",
    "  os.mkdir(PROJECT_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = Session(env_config, event_manager=EventManager())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show info about the environment and a render of the tabletop map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make(\"dndenv-v0\", root_path=\"map_with_obstacles\", show_logs=True,\n",
    "           custom_session=session,\n",
    "           damaged_based_reward=True,\n",
    "           render_mode=\"ansi\")\n",
    "env.reset()\n",
    "print(env.render())\n",
    "print(env.observation_space)\n",
    "print(env.action_space.sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQN Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAJECTORY_POLICY = \"e-greedy\"\n",
    "NUM_UPDATES = 2 # number of training steps to update the Q-network\n",
    "TEMP_DECAY = 0.999\n",
    "BUFFER_CAPACITY = 3000\n",
    "FRAMES_TO_STORE = 2\n",
    "MAX_STEPS = 3000\n",
    "BATCH_SIZE = 64\n",
    "TARGET_UPDATE_FREQ = 1 # how often to update the target network\n",
    "T_HORIZON = 512\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_FINAL = 0.01\n",
    "EPSILON_DECAY_FRAMES = 10**3\n",
    "EVAL_STEPS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = QNetwork(device=device)\n",
    "model.to(device)\n",
    "state, info = env.reset()\n",
    "moves = info[\"available_moves\"]\n",
    "model.eval()\n",
    "print(model(state, moves[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def act_with_policy(state, info, model, policy='e-greedy', temperature=5.0, epsilon=0.1):\n",
    "    available_moves = info[\"available_moves\"]\n",
    "    with torch.no_grad():\n",
    "        if policy == 'boltzmann':\n",
    "            values = torch.stack([model(state, move).squeeze() for move in available_moves])\n",
    "            if len(values) > 1:\n",
    "                if temperature != 0:\n",
    "                    values = values / temperature\n",
    "                else:\n",
    "                    raise ValueError(\"Temperature is zero, which can lead to division by zero.\")\n",
    "\n",
    "                # Stabilizing the exponential calculation\n",
    "                values = values - torch.max(values)  # Subtract the max value for numerical stability\n",
    "                values = torch.exp(values)\n",
    "                sum_values = torch.sum(values)\n",
    "\n",
    "                if sum_values > 0:\n",
    "                    values = values / sum_values\n",
    "                    chosen_index = torch.multinomial(values, 1).item()\n",
    "                else:\n",
    "                    print(\"Sum of exponentiated values is zero. Adjust the model or input.\")\n",
    "                    chosen_index = torch.randint(len(available_moves), (1,)).item()\n",
    "            else:\n",
    "                chosen_index = 0\n",
    "        elif policy == 'e-greedy':\n",
    "            if random.random() < epsilon:\n",
    "                # place available moves in buckets according to their type\n",
    "                # this is so that movements are not chosen more often than other types of moves\n",
    "                move_types = collections.defaultdict(list)\n",
    "                for orig_index, move in enumerate(available_moves):\n",
    "                    move_types[move[0]].append(orig_index)\n",
    "                chosen_move_type = random.choice(list(move_types.keys()))\n",
    "                chosen_index = random.choice(move_types[chosen_move_type])\n",
    "            else:\n",
    "                values = torch.stack([model(state, move) for move in available_moves])\n",
    "                chosen_index = torch.argmax(values).item()\n",
    "        elif policy == 'greedy':\n",
    "                values = torch.stack([model(state, move) for move in available_moves])\n",
    "                chosen_index = torch.argmax(values).item()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown policy: {policy}\")\n",
    "    \n",
    "    return available_moves[chosen_index]\n",
    "\n",
    "def generate_trajectory(env, model, policy='e-greedy', temperature=5.0, epsilon=0.1, horizon=500, quick_exit=False):\n",
    "\n",
    "    done = False\n",
    "    truncated = False\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    dones = []\n",
    "    truncateds = []\n",
    "    infos = []\n",
    "    truncated = False\n",
    "\n",
    "\n",
    "    def reaction_callback(state, reward, done, truncated, info):\n",
    "        action = act_with_policy(state, info, model, policy, temperature, epsilon)\n",
    "        \n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "        truncateds.append(truncated)\n",
    "        infos.append(info)\n",
    "\n",
    "        return action\n",
    "\n",
    "    state, info = env.reset(reaction_callback=reaction_callback)\n",
    "\n",
    "    for _ in range(horizon):\n",
    "        # instead of sampling  (e.g. env.action_space.sample()) we can ask help from the enivronment to obtain valid moves\n",
    "        # as there are sparse valid moves in the environment\n",
    "        action = act_with_policy(state, info, model, policy, temperature, epsilon)\n",
    "        next_state, reward, done, truncated, next_info = env.step(action)\n",
    "\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "        truncateds.append(truncated)\n",
    "        infos.append(info)\n",
    "\n",
    "        if done:\n",
    "            break    \n",
    "        if truncated:\n",
    "            truncated = True\n",
    "            break\n",
    "        state = next_state\n",
    "        info = next_info\n",
    "        \n",
    "    states.append(next_state)\n",
    "    infos.append(next_info)\n",
    "    actions.append((-1, (0,0), (0,0), 0, 0))\n",
    "    return states, actions, rewards, dones, truncateds, infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory = generate_trajectory(env, model, epsilon=1.0)\n",
    "print(trajectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 10\n",
    "total_rewards = 0\n",
    "for i in tqdm.tqdm(range(EPISODES)):\n",
    "    states, actions, rewards, dones, truncateds, infos = generate_trajectory(env, model, epsilon=1.0)\n",
    "    total_rewards += sum(rewards)\n",
    "\n",
    "avg_reward = total_rewards/EPISODES\n",
    "print(f\"Average reward: {avg_reward} Total Reward: {total_rewards}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a batch of trajectories and store them in the replay buffer\n",
    "def generate_batch_trajectories(env, model, n_rollout, replay_buffer: ReplayBuffer, temperature=5.0, epsilon=0.1, horizon=30, policy='e-greedy'):\n",
    "    # print(f\"generating {n_rollout} rollouts\")\n",
    "    for _ in range(n_rollout):\n",
    "        state, action, reward, done, truncated, info = generate_trajectory(env, model, temperature=temperature,\n",
    "                                                                           epsilon=epsilon,\n",
    "                                                                           horizon=horizon,policy=policy)\n",
    "        replay_buffer.push(state, action, reward, info, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, gamma, learning_rate, max_steps=MAX_STEPS, use_td_target=True,\n",
    "          trajectory_policy='e-greedy',\n",
    "          label=\"dnd_egreedy\",\n",
    "          eval_env=None,\n",
    "          reward_per_episode=None,\n",
    "          n_rollout=8,\n",
    "          seed=1337):\n",
    "  print(f\"training with gamma {gamma} and learning rate {learning_rate}\")\n",
    "  env.seed(seed)\n",
    "\n",
    "  replay_buffer = ReplayBuffer(BUFFER_CAPACITY, device)\n",
    "  # load model checkpoint if available\n",
    "  model = QNetwork(device).to(device)\n",
    "  target_model = QNetwork(device).to(device)\n",
    "\n",
    "  # intialize target network with the same weights as the model\n",
    "  target_model.load_state_dict(model.state_dict())\n",
    "\n",
    "  optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "  best_avg = -10\n",
    "  best_step = 0\n",
    "  temperature = 5.0\n",
    "  if reward_per_episode is None:\n",
    "    reward_per_episode = []\n",
    "    \n",
    "  epsilon = EPSILON_START\n",
    "\n",
    "  for step in tqdm.tqdm(range(max_steps)):\n",
    "    generate_batch_trajectories(env, model, n_rollout, replay_buffer, temperature=temperature,\n",
    "                                epsilon=epsilon, policy=trajectory_policy, horizon=T_HORIZON)\n",
    "\n",
    "    states, actions, rewards, infos, is_terminals = replay_buffer.sample(BATCH_SIZE)\n",
    "    rewards_collected = 0\n",
    "    for _ in range(NUM_UPDATES):\n",
    "      rewards_collected = 0\n",
    "      total_loss = 0.0\n",
    "\n",
    "      for i in range(len(states)):\n",
    "        s = states[i]\n",
    "        a = actions[i]\n",
    "        env_info = infos[i]\n",
    "        r = torch.tensor(rewards[i]).to(device).unsqueeze(1)\n",
    "        is_terminal = torch.tensor(is_terminals[i]).float().to(device).unsqueeze(1)\n",
    "\n",
    "        if use_td_target:\n",
    "          with torch.no_grad():\n",
    "            s_next = s[1:]\n",
    "            a_next = a[1:]\n",
    "            q_targets = target_model.forward(s_next, a_next, pre_converted=True, pre_converted_action=True).detach()\n",
    "        else: # Q-learning target == \"slow\"\n",
    "          with torch.no_grad():\n",
    "            s_next = s[1:]\n",
    "            s_info = env_info[1:]\n",
    "            q_targets = torch.zeros(len(s_next)).to(device)\n",
    "\n",
    "            for index in range(len(s_info)):\n",
    "              info = s_info[index]\n",
    "              state = s_next[index]\n",
    "\n",
    "              if len(state) == 0:\n",
    "                q_targets[index] = 0\n",
    "                continue\n",
    "\n",
    "              total_available_moves = len(info[\"available_moves\"])\n",
    "              states_t = [state] * total_available_moves\n",
    "              avail_actions = info[\"available_moves\"]\n",
    "              assert len(states_t) > 0, \"No available states\"\n",
    "              assert len(avail_actions) > 0, \"No available moves\"\n",
    "\n",
    "              q_values = target_model.forward(states_t, avail_actions, pre_converted=True).detach().squeeze(1)\n",
    "              if len(q_values) == 0:\n",
    "                q_targets[index] = 0\n",
    "              else:\n",
    "                q_targets[index] = torch.max(q_values).item()\n",
    "\n",
    "            q_targets = q_targets.unsqueeze(1)\n",
    "            assert q_targets.shape == r.shape, f\"q_targets shape {q_targets.shape} != r shape {r.shape}\"\n",
    "\n",
    "        targets = r + gamma * q_targets * (1 - is_terminal)\n",
    "\n",
    "        s_input = s[0:-1]\n",
    "        a_input = a[0:-1]\n",
    "        output = model.forward(s_input, a_input, pre_converted=True, pre_converted_action=True)\n",
    "        q_sa = output\n",
    "\n",
    "        value_loss = nn.MSELoss()(q_sa, targets)\n",
    "        optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        total_loss += value_loss.item()\n",
    "        rewards_collected += r.sum().item()\n",
    "        optimizer.step()\n",
    "\n",
    "    # save model checkpoint\n",
    "\n",
    "    # evaluate model performance\n",
    "    if step % 10 == 0:\n",
    "      if eval_env is None:\n",
    "        eval_env = env\n",
    "\n",
    "      eval_rewards = []\n",
    "      for _ in range(EVAL_STEPS):\n",
    "        _, _, rewards, _, _, _ = generate_trajectory(eval_env, model, policy='greedy')\n",
    "        total_reward = sum(rewards)\n",
    "        eval_rewards.append(total_reward)\n",
    "\n",
    "      avg_rewards = np.mean(eval_rewards)\n",
    "     \n",
    "      # print(f\"eval rewards: {avg_rewards}\")\n",
    "      reward_per_episode.append(avg_rewards)\n",
    "      std_rewards = np.std(reward_per_episode)\n",
    "\n",
    "      # print(f\"total reward: {total_reward}\")\n",
    "      if trajectory_policy == \"e-greedy\":\n",
    "        print(f\"{step}: avg rewards {avg_rewards} std: {std_rewards} best avg {best_avg}@{best_step} epsilon {epsilon}\")\n",
    "      elif trajectory_policy == \"boltzmann\":\n",
    "        print(f\"{step}: avg rewards {avg_rewards} std: {std_rewards} best avg {best_avg}@{best_step} temperature {temperature}\")\n",
    "      else:\n",
    "        print(f\"{step}: avg rewards {avg_rewards} std: {std_rewards} best avg {best_avg}@{best_step}\")\n",
    "\n",
    "      replay_buffer.print_stats()\n",
    "\n",
    "      if avg_rewards > best_avg:\n",
    "        print(f\"best: {avg_rewards}\")\n",
    "        best_avg = avg_rewards\n",
    "        best_step = step\n",
    "        torch.save(model.state_dict(), f\"{PROJECT_OUTPUT_PATH}/model_best_{label}@{step}.pt\")\n",
    "        torch.save(model.state_dict(), f\"{PROJECT_OUTPUT_PATH}/model_best_{label}.pt\")\n",
    "\n",
    "      # torch.save(model.state_dict(), f\"model_{label}_{gamma}_{learning_rate}.pt\")\n",
    "\n",
    "\n",
    "    # if step % 100 == 0:\n",
    "    #   torch.save(model.state_dict(), f\"model_{label}_{gamma}_{learning_rate}_{step}.pt\")\n",
    "\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "    # decay temp\n",
    "    temperature = np.max([0.1, temperature * TEMP_DECAY])\n",
    "\n",
    "    # decay epsilon\n",
    "    epsilon = EPSILON_FINAL + (EPSILON_START - EPSILON_FINAL) * np.exp(-1.0 * step / EPSILON_DECAY_FRAMES)\n",
    "\n",
    "    if step % TARGET_UPDATE_FREQ == 0:\n",
    "      # calculate the avg change weights of the model with the target model\n",
    "      total_change = 0\n",
    "      for p, p_target in zip(model.parameters(), target_model.parameters()):\n",
    "        total_change += torch.abs(p - p_target).sum().item()\n",
    "      # print(f\"total change: {total_change}\")\n",
    "\n",
    "      target_model.load_state_dict(model.state_dict())\n",
    "\n",
    "  env.close()\n",
    "  return reward_per_episode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the location of the game configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_setup_path = \"map_with_obstacles\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the env setup. Note that we use damaged based rewards to give a denser reward signalling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(root_path, render_mode=\"ansi\", show_logs=False, custom_agent=None):\n",
    "    return make(\"dndenv-v0\", root_path=root_path, show_logs=show_logs,\n",
    "                render_mode=render_mode,\n",
    "                damage_based_reward=True,\n",
    "                custom_agent=custom_agent,\n",
    "                profiles=lambda: random.choice(['high_elf_fighter', 'high_elf_mage', 'dwarf_cleric', 'halfling_rogue']),\n",
    "                enemies=lambda: random.choice(['high_elf_fighter', 'high_elf_mage', 'dwarf_cleric', 'halfling_rogue']),\n",
    "                map_file=lambda: random.choice(['maps/simple_map',\\\n",
    "                                                'maps/complex_map', \\\n",
    "                                                'maps/game_map', \\\n",
    "                                                'maps/walled_map'])\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(game_setup_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1337\n",
    "# Create a grid of learning rates and gammas\n",
    "learning_rates = [0.0001]\n",
    "gammas = [0.99]\n",
    "\n",
    "results = {}\n",
    "for lr in learning_rates:\n",
    "  results[lr] = {}\n",
    "  for gamma in gammas:\n",
    "    seed = seed + 1\n",
    "    reward_per_episode = train(env, gamma, lr, max_steps=MAX_STEPS, seed=seed, use_td_target=True)\n",
    "    results[lr][gamma] = reward_per_episode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize rewards per episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in results:\n",
    "  for gamma in results[item]:\n",
    "    print(f\"lr: {item} gamma: {gamma} rewards: {results[item][gamma]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for gamma in gammas:\n",
    "        plt.plot(results[lr][gamma], label=f\"lr={lr}, gamma={gamma}\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup a mixed LLM vs Rules based AI adversary and compare performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM related config, like which LLM to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_OPENAI=False\n",
    "LLAMA3_BASE_URL= os.environ.get('LLAMA3_BASE_URL', 'http://localhost:8000/v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = GPT4Interfacer(debug=False, tools=False, base_url=LLAMA3_BASE_URL, api_key=\"token1234\", variant=\"NousResearch/Meta-Llama-3.1-8B-Instruct\", explain=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup interface for the adversarial moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomAgent:\n",
    "    def action(self, observation, info):\n",
    "        return self.llm_interface.select_action_for_state(observation, info)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return \"Custom LLM Agent\"\n",
    "\n",
    "agent = CustomAgent(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make an evaluation environment and a training environment. The evaluation environment is the same env config as the previous training run, while the training environment incorporates the LLM as the adversary. We do this to compare the training results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natural20.generic_controller import GenericController\n",
    "from natural20.gym.dndenv_controller import DndenvController\n",
    "\n",
    "eval_env = make_env(game_setup_path)\n",
    "\n",
    "def mix_controller(session, mix=0.8, **kwargs):\n",
    "    # 80% of the time use the generic controller, 20% LLM\n",
    "    if (random.random() < mix):\n",
    "        return GenericController(session)\n",
    "    else:\n",
    "        return DndenvController(session, agent)\n",
    "\n",
    "env = make_env(game_setup_path, custom_agent=mix_controller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1337\n",
    "# Create a grid of learning rates and gammas\n",
    "learning_rates = [0.001]\n",
    "gammas = [0.99]\n",
    "\n",
    "results_2 = {}\n",
    "for lr in learning_rates:\n",
    "  results_2[lr] = {}\n",
    "  for gamma in gammas:\n",
    "    seed = seed + 1\n",
    "    reward_per_episode = train(env, gamma, lr, max_steps=MAX_STEPS, seed=seed, \\\n",
    "                               eval_env=eval_env, trajectory_policy='e-greedy', label=\"llm_adversary\")\n",
    "    results_2[lr][gamma] = reward_per_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in results_2:\n",
    "  for gamma in results_2[item]:\n",
    "    print(f\"lr: {item} gamma: {gamma} rewards: {results_2[item][gamma]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for gamma in gammas:\n",
    "        plt.plot(results_2[lr][gamma], label=f\"lr={lr}, gamma={gamma}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure prompter for mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VARIANT = 'mistralai/Mistral-7B-Instruct-v0.3'\n",
    "MISTRAL_URL = os.environ.get('MISTRAL_URL', \"http://localhost:8000/v1\")\n",
    "prompt_mistral = GPT4Interfacer(debug=False, tools=False, base_url=MISTRAL_URL, api_key=\"token1234\", variant=VARIANT, explain=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomAgent:\n",
    "    def __init__(self, llm_interface):\n",
    "        self.llm_interface = llm_interface\n",
    "\n",
    "    def action(self, observation, info):\n",
    "        return self.llm_interface.select_action_for_state(observation, info)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return \"Custom LLM Agent\"\n",
    "\n",
    "agent_mistral = CustomAgent(prompt_mistral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natural20.generic_controller import GenericController\n",
    "from natural20.gym.dndenv_controller import DndenvController\n",
    "from llm_interface import GPT4Interfacer\n",
    "\n",
    "eval_env = make_env(game_setup_path)\n",
    "\n",
    "def mix_controller_mistral(session, mix=0.8, **kwargs):\n",
    "    # 80% of the time use the generic controller, 20% LLM\n",
    "    if (random.random() < mix):\n",
    "        return GenericController(session)\n",
    "    else:\n",
    "        return DndenvController(session, agent_mistral)\n",
    "\n",
    "env = make_env(game_setup_path, custom_agent=mix_controller_mistral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1337\n",
    "# Create a grid of learning rates and gammas\n",
    "learning_rates = [0.001]\n",
    "gammas = [0.99]\n",
    "\n",
    "results_4 = {}\n",
    "for lr in learning_rates:\n",
    "  results_4[lr] = {}\n",
    "  for gamma in gammas:\n",
    "    seed = seed + 1\n",
    "    reward_per_episode = train(env, gamma, lr, max_steps=MAX_STEPS, seed=seed, \\\n",
    "                               eval_env=eval_env, trajectory_policy='e-greedy', label=\"llm_adversary_mistral\")\n",
    "    results_4[lr][gamma] = reward_per_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in results_4:\n",
    "  for gamma in results_4[item]:\n",
    "    print(f\"lr: {item} gamma: {gamma} rewards: {results_4[item][gamma]}\")\n",
    "\n",
    "# show the best reward found\n",
    "print(f\"Best reward: {max(results_4[0.001][0.99])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for gamma in gammas:\n",
    "        plt.plot(results_4[lr][gamma], label=f\"lr={lr}, gamma={gamma}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure OpenAI GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VARIANT = 'gpt-4o-mini'\n",
    "BACKUP_VARIANT = 'NousResearch/Meta-Llama-3.1-8B-Instruct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt_gpt4 = GPT4Interfacer(debug=False, tools=True, api_key=\"OPENAI_TOKEN\", variant=VARIANT, explain=False)\n",
    "prompt_backup = GPT4Interfacer(debug=False, tools=False, base_url=LLAMA3_BASE_URL, api_key=\"token1234\", variant=BACKUP_VARIANT)\n",
    "\n",
    "class CustomAgentGPT4:\n",
    "    \"\"\"\n",
    "    Custom agent that uses the GPT-4 model for action selection.\n",
    "    Uses a backup LLM for movement and free action selection.\n",
    "    \"\"\"\n",
    "    def __init__(self, llm_interface, backup_interface=None):\n",
    "        self.llm_interface = llm_interface\n",
    "        self.backup_interface = backup_interface\n",
    "        self.invocation_count = 0\n",
    "        self.backup_invocation_count = 0\n",
    "\n",
    "    def action(self, observation, info):\n",
    "        action, bonus_action, _ =  observation[\"turn_info\"]\n",
    "        if self.backup_interface is not None and action==0:\n",
    "            self.backup_invocation_count += 1\n",
    "            return self.backup_interface.select_action_for_state(observation, info)\n",
    "\n",
    "        self.invocation_count += 1\n",
    "        return self.llm_interface.select_action_for_state(observation, info)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return \"Custom LLM Agent\"\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return \"Custom LLM Agent\"\n",
    "\n",
    "agent_gpt4 = CustomAgentGPT4(prompt_gpt4, backup_interface=prompt_backup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natural20.generic_controller import GenericController\n",
    "from natural20.gym.dndenv_controller import DndenvController\n",
    "\n",
    "eval_env = make_env(game_setup_path)\n",
    "\n",
    "def mix_controller_gpt4(session, mix=0.8, **kwargs):\n",
    "    # 80% of the time use the generic controller, 20% LLM\n",
    "    if (random.random() < mix):\n",
    "        return GenericController(session)\n",
    "    else:\n",
    "        return DndenvController(session, agent_gpt4)\n",
    "\n",
    "env = make_env(game_setup_path, custom_agent=mix_controller_gpt4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1337\n",
    "# Create a grid of learning rates and gammas\n",
    "learning_rates = [0.001]\n",
    "gammas = [0.99]\n",
    "\n",
    "results_5 = {}\n",
    "for lr in learning_rates:\n",
    "  results_5[lr] = {}\n",
    "  for gamma in gammas:\n",
    "    seed = seed + 1\n",
    "    results_5[lr][gamma] = []\n",
    "    reward_per_episode = train(env, gamma, lr, max_steps=MAX_STEPS, seed=seed, \\\n",
    "                               reward_per_episode=results_5[lr][gamma],\n",
    "                               eval_env=eval_env, trajectory_policy='e-greedy', label=\"llm_adversary_gpt4\")\n",
    "    results_5[lr][gamma] = reward_per_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in results_5:\n",
    "  for gamma in results_5[item]:\n",
    "    print(f\"lr: {item} gamma: {gamma} rewards: {results_5[item][gamma]}\")\n",
    "\n",
    "# show the best reward found\n",
    "print(f\"Best reward: {max(results_5[0.001][0.99])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for gamma in gammas:\n",
    "        plt.plot(results_5[lr][gamma], label=f\"lr={lr}, gamma={gamma}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VARIANT = 'gpt-4o'\n",
    "BACKUP_VARIANT = 'NousResearch/Meta-Llama-3.1-8B-Instruct'\n",
    "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_gpt4 = GPT4Interfacer(debug=False, tools=True, api_key=OPENAI_API_KEY, variant=VARIANT, explain=False)\n",
    "agent_gpt4 = CustomAgentGPT4(prompt_gpt4, backup_interface=prompt_backup)\n",
    "\n",
    "from natural20.generic_controller import GenericController\n",
    "from natural20.gym.dndenv_controller import DndenvController\n",
    "\n",
    "eval_env = make_env(game_setup_path)\n",
    "\n",
    "def mix_controller_gpt4(session, mix=0.8, **kwargs):\n",
    "    # 80% of the time use the generic controller, 20% LLM\n",
    "    if (random.random() < mix):\n",
    "        return GenericController(session)\n",
    "    else:\n",
    "        return DndenvController(session, agent_gpt4)\n",
    "\n",
    "env = make_env(game_setup_path, custom_agent=mix_controller_gpt4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1337\n",
    "# # Create a grid of learning rates and gammas\n",
    "# learning_rates = [0.001]\n",
    "# gammas = [0.99]\n",
    "\n",
    "# results_6 = {}\n",
    "# reward_per_episode = []\n",
    "# for lr in learning_rates:\n",
    "#   results_6[lr] = {}\n",
    "#   for gamma in gammas:\n",
    "#     seed = seed + 1\n",
    "#     results_6[lr][gamma] = []\n",
    "#     reward_per_episode = train(env, gamma, lr, max_steps=MAX_STEPS, seed=seed, \\\n",
    "#                                reward_per_episode=results_6[lr][gamma],\n",
    "#                                eval_env=eval_env, trajectory_policy='e-greedy', label=\"llm_adversary_gpt4_2\")\n",
    "#     results_6[lr][gamma] = reward_per_episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize best Avg. Rewards obtained from each training method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "results[0.001] = {}\n",
    "results[0.001][0.99] = [-7.634523809523809, -3.7698412698412698, -2.4611111111111112, 0.12420634920634925, -4.1460317460317455, -3.848412698412699, -1.9047619047619049, 1.5638888888888889, -1.4793650793650794, 0.19523809523809518, -1.801984126984127, 2.0448412698412697, 0.030158730158729982, 0.8888888888888887, 1.3702380952380953, 3.7460317460317456, 1.925, -1.6642857142857141, 2.0055555555555555, 1.753968253968254, 1.8, 1.5722222222222222, 2.4007936507936507, 4.449999999999999, 3.3579365079365084, 1.650793650793651, 1.9138888888888885, 4.8388888888888895, 1.8265873015873018, 0.757936507936508, 2.0436507936507935, -0.9964285714285714, 1.2666666666666668, 3.8055555555555554, 3.211111111111111, 0.1599206349206352, 0.38888888888888873, 2.65, 2.2079365079365085, 0.7555555555555554, 5.116666666666667, 1.7682539682539684, 2.522222222222222, 3.7888888888888888, 4.283333333333333, 3.4333333333333336, 3.2999999999999994, 2.354761904761905, 0.5055555555555555, 0.041666666666666644, 3.0980158730158736, 4.307936507936508, 3.40952380952381, 2.3083333333333336, 1.9777777777777776, 2.0888888888888886, 2.8809523809523814, 0.9626984126984126, 2.7999999999999994, 5.114285714285714, 0.24246031746031732, 3.538492063492064, 3.6317460317460313, 3.8337301587301584, 3.584920634920635, 2.906746031746031, 4.142857142857142, 0.9460317460317461, 4.44404761904762, 3.226190476190476, 3.2, 3.3317460317460315, 3.271428571428571, 1.1944444444444442, 3.425396825396825, 2.7376984126984127, 5.161904761904761, 3.8416666666666663, 3.501587301587301, 1.734920634920635, 2.083333333333333, 3.7476190476190476, 3.2904761904761903, 5.750396825396825, 4.102777777777778, 2.8706349206349207, 2.6519841269841264, 3.9178571428571427, 1.575, 3.088095238095238, 4.911111111111111, 3.738888888888889, 3.4444444444444446, 5.966666666666666, 3.6869047619047617, 5.313888888888889, 5.48452380952381, 4.851984126984127, 2.298809523809524, 4.462301587301587]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_per_method = {}\n",
    "for item in results:\n",
    "  for gamma in results[item]:\n",
    "    rewards_per_method[\"rules_based\"] = np.max(results[item][gamma])\n",
    "for item in results_2:\n",
    "  for gamma in results_2[item]:\n",
    "    rewards_per_method[\"llm_llama3\"] = np.max(results_2[item][gamma])\n",
    "for item in results_4:\n",
    "    for gamma in results_4[item]:\n",
    "        rewards_per_method[\"llm_mistral\"] = np.max(results_4[item][gamma])\n",
    "for item in results_5:\n",
    "    for gamma in results_5[item]:\n",
    "        rewards_per_method[\"llm_gpt4o\"] = np.max(results_5[item][gamma])\n",
    "\n",
    "# show a table of the best rewards from each method:\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(rewards_per_method.items(), columns=[\"method\", \"rewards\" ])\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform some tests on the trained agent. Show a combat log from a fight against the rules based AI. Define a policy based on the Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "MAX_STEPS = 500\n",
    "NUM_EPISODES = 100\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class ModelPolicy:\n",
    "    def __init__(self, weights_file = 'model_best_dnd_egreedy.pt'):\n",
    "        self.model = QNetwork(device=device)\n",
    "        self.model.to(device)\n",
    "        if not os.path.exists(weights_file):\n",
    "            raise FileNotFoundError(f\"Model file {weights_file} not found. Please run dnd_dqn.ipynb notebook to train an agent.\")\n",
    "        self.model.load_state_dict(torch.load(weights_file))\n",
    "\n",
    "    def action(self, state, info):\n",
    "        available_moves = info[\"available_moves\"]\n",
    "        values = torch.stack([self.model(state, move) for move in available_moves])\n",
    "        for index, v in enumerate(values):\n",
    "            print(f\"{index}: {available_moves[index]} {v.item()}\")\n",
    "\n",
    "        chosen_index = torch.argmax(values).item()\n",
    "        return available_moves[chosen_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = make_env(game_setup_path)\n",
    "\n",
    "print(\"=========================================\")\n",
    "print(\"Battle between an RL agent vs a Rules based AI\")\n",
    "print(\"=========================================\")\n",
    "win_count = 0\n",
    "loss_count = 0\n",
    "for i in range(NUM_EPISODES):\n",
    "    observation, info = env.reset()\n",
    "    model = ModelPolicy()\n",
    "    action = action = model.action(observation, info)\n",
    "\n",
    "    print(f\"selected action: {action}\")\n",
    "    terminal = False\n",
    "    episode = 0\n",
    "\n",
    "    while not terminal and episode < MAX_STEPS:\n",
    "        episode += 1\n",
    "        observation, reward, terminal, truncated, info = env.step(action)\n",
    "        print(env.render())\n",
    "        if not terminal and not truncated:\n",
    "            episode_name_with_padding = str(episode).zfill(3)\n",
    "\n",
    "            # display entity healths\n",
    "            print(f\"Turn {info['current_index']}\\n\")\n",
    "            print(f\"Reward: {reward}\\n\")\n",
    "            print(f\"health hero: {observation['health_pct']}\\n\")\n",
    "            print(f\"health enemy: {observation['health_enemy']}\\n\")\n",
    "            print(env.render())\n",
    "            \n",
    "            action = model.action(observation, info)\n",
    "            print(f\"agent selected action: {action}\")\n",
    "\n",
    "        if terminal or truncated:\n",
    "            print(f\"Reward: {reward}\")\n",
    "            if reward > 0:\n",
    "                win_count += 1\n",
    "            else:\n",
    "                loss_count += 1\n",
    "            break\n",
    "        \n",
    "print(f\"Win count: {win_count} Loss count: {loss_count} Win rate: {win_count/(win_count+loss_count)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "natural_20",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
