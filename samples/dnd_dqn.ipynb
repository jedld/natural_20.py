{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyyaml in /home/jedld/.local/lib/python3.10/site-packages (6.0.1)\n",
      "Requirement already satisfied: dndice in /home/jedld/.local/lib/python3.10/site-packages (2.8.1)\n",
      "Requirement already satisfied: python-i18n in /home/jedld/.local/lib/python3.10/site-packages (0.3.9)\n",
      "Requirement already satisfied: gymnasium in /home/jedld/.local/lib/python3.10/site-packages (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/jedld/miniconda3/envs/ai322/lib/python3.10/site-packages (from gymnasium) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/jedld/.local/lib/python3.10/site-packages (from gymnasium) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/jedld/.local/lib/python3.10/site-packages (from gymnasium) (4.9.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/jedld/.local/lib/python3.10/site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: inflect in /home/jedld/.local/lib/python3.10/site-packages (7.0.0)\n",
      "Requirement already satisfied: pydantic>=1.9.1 in /home/jedld/miniconda3/envs/ai322/lib/python3.10/site-packages (from inflect) (2.7.1)\n",
      "Requirement already satisfied: typing-extensions in /home/jedld/.local/lib/python3.10/site-packages (from inflect) (4.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/jedld/.local/lib/python3.10/site-packages (from pydantic>=1.9.1->inflect) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /home/jedld/miniconda3/envs/ai322/lib/python3.10/site-packages (from pydantic>=1.9.1->inflect) (2.18.2)\n",
      "Requirement already satisfied: collections-extended in /home/jedld/miniconda3/envs/ai322/lib/python3.10/site-packages (2.0.2)\n",
      "Requirement already satisfied: openai in /home/jedld/miniconda3/envs/ai322/lib/python3.10/site-packages (1.30.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/jedld/.local/lib/python3.10/site-packages (from openai) (3.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/jedld/miniconda3/envs/ai322/lib/python3.10/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/jedld/miniconda3/envs/ai322/lib/python3.10/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/jedld/miniconda3/envs/ai322/lib/python3.10/site-packages (from openai) (2.7.1)\n",
      "Requirement already satisfied: sniffio in /home/jedld/.local/lib/python3.10/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /home/jedld/.local/lib/python3.10/site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/jedld/.local/lib/python3.10/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: idna>=2.8 in /home/jedld/.local/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (2.10)\n",
      "Requirement already satisfied: exceptiongroup in /home/jedld/.local/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.1.1)\n",
      "Requirement already satisfied: certifi in /home/jedld/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2023.5.7)\n",
      "Requirement already satisfied: httpcore==1.* in /home/jedld/miniconda3/envs/ai322/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/jedld/miniconda3/envs/ai322/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/jedld/.local/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /home/jedld/miniconda3/envs/ai322/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.18.2)\n",
      "Obtaining file:///home/jedld/workspace/natural_20.py\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /home/jedld/.local/lib/python3.10/site-packages (from natural20.py==0.1) (6.0.1)\n",
      "Requirement already satisfied: dndice in /home/jedld/.local/lib/python3.10/site-packages (from natural20.py==0.1) (2.8.1)\n",
      "Requirement already satisfied: python-i18n in /home/jedld/.local/lib/python3.10/site-packages (from natural20.py==0.1) (0.3.9)\n",
      "Requirement already satisfied: fantasynames in /home/jedld/miniconda3/envs/ai322/lib/python3.10/site-packages (from natural20.py==0.1) (0.1.2)\n",
      "Requirement already satisfied: black<21.0,>=20.8b1 in /home/jedld/miniconda3/envs/ai322/lib/python3.10/site-packages (from fantasynames->natural20.py==0.1) (20.8b1)\n",
      "Requirement already satisfied: flake8<4.0.0,>=3.9.0 in /home/jedld/miniconda3/envs/ai322/lib/python3.10/site-packages (from fantasynames->natural20.py==0.1) (3.9.2)\n",
      "Requirement already satisfied: mypy<0.813,>=0.812 in /home/jedld/miniconda3/envs/ai322/lib/python3.10/site-packages (from fantasynames->natural20.py==0.1) (0.812)\n",
      "Requirement already satisfied: pytest<7.0.0,>=6.2.3 in /home/jedld/miniconda3/envs/ai322/lib/python3.10/site-packages (from fantasynames->natural20.py==0.1) (6.2.5)\n",
      "Requirement already satisfied: click>=7.1.2 in /home/jedld/.local/lib/python3.10/site-packages (from black<21.0,>=20.8b1->fantasynames->natural20.py==0.1) (8.1.7)\n",
      "Requirement already satisfied: appdirs in /home/jedld/miniconda3/envs/ai322/lib/python3.10/site-packages (from black<21.0,>=20.8b1->fantasynames->natural20.py==0.1) (1.4.4)\n",
      "Requirement already satisfied: toml>=0.10.1 in /home/jedld/miniconda3/envs/ai322/lib/python3.10/site-packages (from black<21.0,>=20.8b1->fantasynames->natural20.py==0.1) (0.10.2)\n",
      "Requirement already satisfied: typed-ast>=1.4.0 in /home/jedld/miniconda3/envs/ai322/lib/python3.10/site-packages (from black<21.0,>=20.8b1->fantasynames->natural20.py==0.1) (1.4.3)\n",
      "Requirement already satisfied: regex>=2020.1.8 in /home/jedld/.local/lib/python3.10/site-packages (from black<21.0,>=20.8b1->fantasynames->natural20.py==0.1) (2023.12.25)\n",
      "Requirement already satisfied: pathspec<1,>=0.6 in /home/jedld/miniconda3/envs/ai322/lib/python3.10/site-packages (from black<21.0,>=20.8b1->fantasynames->natural20.py==0.1) (0.12.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /home/jedld/.local/lib/python3.10/site-packages (from black<21.0,>=20.8b1->fantasynames->natural20.py==0.1) (4.9.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in /home/jedld/miniconda3/envs/ai322/lib/python3.10/site-packages (from black<21.0,>=20.8b1->fantasynames->natural20.py==0.1) (0.4.4)\n",
      "Requirement already satisfied: pyflakes<2.4.0,>=2.3.0 in /home/jedld/miniconda3/envs/ai322/lib/python3.10/site-packages (from flake8<4.0.0,>=3.9.0->fantasynames->natural20.py==0.1) (2.3.1)\n",
      "Requirement already satisfied: pycodestyle<2.8.0,>=2.7.0 in /home/jedld/miniconda3/envs/ai322/lib/python3.10/site-packages (from flake8<4.0.0,>=3.9.0->fantasynames->natural20.py==0.1) (2.7.0)\n",
      "Requirement already satisfied: mccabe<0.7.0,>=0.6.0 in /home/jedld/miniconda3/envs/ai322/lib/python3.10/site-packages (from flake8<4.0.0,>=3.9.0->fantasynames->natural20.py==0.1) (0.6.1)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /home/jedld/miniconda3/envs/ai322/lib/python3.10/site-packages (from pytest<7.0.0,>=6.2.3->fantasynames->natural20.py==0.1) (23.2.0)\n",
      "Requirement already satisfied: iniconfig in /home/jedld/miniconda3/envs/ai322/lib/python3.10/site-packages (from pytest<7.0.0,>=6.2.3->fantasynames->natural20.py==0.1) (2.0.0)\n",
      "Requirement already satisfied: packaging in /home/jedld/.local/lib/python3.10/site-packages (from pytest<7.0.0,>=6.2.3->fantasynames->natural20.py==0.1) (23.2)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /home/jedld/.local/lib/python3.10/site-packages (from pytest<7.0.0,>=6.2.3->fantasynames->natural20.py==0.1) (1.5.0)\n",
      "Requirement already satisfied: py>=1.8.2 in /home/jedld/miniconda3/envs/ai322/lib/python3.10/site-packages (from pytest<7.0.0,>=6.2.3->fantasynames->natural20.py==0.1) (1.11.0)\n",
      "Installing collected packages: natural20.py\n",
      "  Attempting uninstall: natural20.py\n",
      "    Found existing installation: natural20.py 0.1\n",
      "    Uninstalling natural20.py-0.1:\n",
      "      Successfully uninstalled natural20.py-0.1\n",
      "  Running setup.py develop for natural20.py\n",
      "Successfully installed natural20.py\n",
      "Requirement already satisfied: ipywidgets in /home/jedld/miniconda3/envs/ai322/lib/python3.10/site-packages (8.1.3)\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/jedld/miniconda3/envs/ai322/lib/python3.10/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /home/jedld/miniconda3/envs/ai322/lib/python3.10/site-packages (from ipywidgets) (8.22.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/jedld/miniconda3/envs/ai322/lib/python3.10/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.11 in /home/jedld/miniconda3/envs/ai322/lib/python3.10/site-packages (from ipywidgets) (4.0.11)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.11 in /home/jedld/miniconda3/envs/ai322/lib/python3.10/site-packages (from ipywidgets) (3.0.11)\n",
      "Requirement already satisfied: decorator in /home/jedld/miniconda3/envs/ai322/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/jedld/miniconda3/envs/ai322/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /home/jedld/miniconda3/envs/ai322/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /home/jedld/miniconda3/envs/ai322/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.42)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/jedld/miniconda3/envs/ai322/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /home/jedld/miniconda3/envs/ai322/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
      "Requirement already satisfied: exceptiongroup in /home/jedld/.local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/jedld/miniconda3/envs/ai322/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /home/jedld/miniconda3/envs/ai322/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/jedld/miniconda3/envs/ai322/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/jedld/miniconda3/envs/ai322/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/jedld/miniconda3/envs/ai322/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/jedld/miniconda3/envs/ai322/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /home/jedld/miniconda3/envs/ai322/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/jedld/miniconda3/envs/ai322/lib/python3.10/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Requirement already satisfied: iprogress in /home/jedld/miniconda3/envs/ai322/lib/python3.10/site-packages (0.4)\n",
      "Requirement already satisfied: six in /home/jedld/miniconda3/envs/ai322/lib/python3.10/site-packages (from iprogress) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyyaml\n",
    "!pip install dndice\n",
    "!pip install python-i18n\n",
    "!pip install gymnasium\n",
    "!pip install inflect\n",
    "!pip install collections-extended\n",
    "!pip install openai\n",
    "!pip install -e ..\n",
    "!pip install ipywidgets\n",
    "!pip install iprogress\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium import make\n",
    "from model import QNetwork\n",
    "from natural20.gym.dndenv import dndenv\n",
    "import torch\n",
    "import tqdm as tqdm\n",
    "import tqdm.notebook as tqdm\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import gc\n",
    "import numpy as np\n",
    "import sys\n",
    "import collections\n",
    "from natural20.utils.utils import Session\n",
    "from natural20.event_manager import EventManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "  device = torch.device(\"mps\")\n",
    "else:\n",
    "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config = \"map_with_obstacles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = Session(env_config, event_manager=EventManager())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show info about the environment and a render of the tabletop map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading map from map_with_obstacles/maps/game_map.yml\n",
      "Creating new event manager\n",
      "==== Player Character ====\n",
      "name: gomerin\n",
      "level: 1\n",
      "character class: {'rogue': 1}\n",
      "hp: 18\n",
      "max hp: 18\n",
      "ac: 16\n",
      "speed: 25\n",
      "\n",
      "\n",
      "\n",
      "==== Player Character ====\n",
      "name: rumblebelly\n",
      "level: 1\n",
      "character class: {'rogue': 1}\n",
      "hp: 18\n",
      "max hp: 18\n",
      "ac: 16\n",
      "speed: 25\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'PlayerCharacter' object has no attribute 'proficient_in'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m env \u001b[38;5;241m=\u001b[39m make(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdndenv-v0\u001b[39m\u001b[38;5;124m\"\u001b[39m, root_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmap_with_obstacles\u001b[39m\u001b[38;5;124m\"\u001b[39m, show_logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      2\u001b[0m            custom_session\u001b[38;5;241m=\u001b[39msession,\n\u001b[1;32m      3\u001b[0m            damaged_based_reward\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      4\u001b[0m            render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mansi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(env\u001b[38;5;241m.\u001b[39mrender())\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(env\u001b[38;5;241m.\u001b[39mobservation_space)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/wrappers/order_enforcing.py:61\u001b[0m, in \u001b[0;36mOrderEnforcing.reset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Resets the environment with `kwargs`.\"\"\"\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/wrappers/env_checker.py:57\u001b[0m, in \u001b[0;36mPassiveEnvChecker.reset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_reset:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_reset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43menv_reset_passive_checker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mreset(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:186\u001b[0m, in \u001b[0;36menv_reset_passive_checker\u001b[0;34m(env, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdeprecation(\n\u001b[1;32m    182\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    183\u001b[0m     )\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# Checks the result of env.reset with kwargs\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    189\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    190\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(result)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    191\u001b[0m     )\n",
      "File \u001b[0;32m~/workspace/natural_20.py/natural20/gym/dndenv.py:205\u001b[0m, in \u001b[0;36mdndenv.reset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbattle\u001b[38;5;241m.\u001b[39mcurrent_turn()\u001b[38;5;241m.\u001b[39mreset_turn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbattle)\n\u001b[1;32m    203\u001b[0m current_player \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbattle\u001b[38;5;241m.\u001b[39mcurrent_turn()\n\u001b[0;32m--> 205\u001b[0m current_player, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_game_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_player\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m# get the first player which is not the same group as the current one\u001b[39;00m\n\u001b[1;32m    208\u001b[0m observation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_observation(current_player)\n",
      "File \u001b[0;32m~/workspace/natural_20.py/natural20/gym/dndenv.py:250\u001b[0m, in \u001b[0;36mdndenv._game_loop\u001b[0;34m(self, current_player)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno move for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_player\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbattle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbattle\u001b[38;5;241m.\u001b[39mcommit(action)\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbattle\u001b[38;5;241m.\u001b[39mbattle_ends():\n",
      "File \u001b[0;32m~/workspace/natural_20.py/natural20/battle.py:231\u001b[0m, in \u001b[0;36mBattle.action\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maction\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m    228\u001b[0m     opts \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    229\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbattle\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m    230\u001b[0m     }\n\u001b[0;32m--> 231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43maction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresolve\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/natural_20.py/natural20/actions/look_action.py:27\u001b[0m, in \u001b[0;36mLookAction.resolve\u001b[0;34m(self, session, map, opts)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresolve\u001b[39m(\u001b[38;5;28mself\u001b[39m, session, \u001b[38;5;28mmap\u001b[39m, opts\u001b[38;5;241m=\u001b[39m{}):\n\u001b[0;32m---> 27\u001b[0m     perception_check \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperception_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbattle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     perception_check_2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource\u001b[38;5;241m.\u001b[39mperception_check(opts[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbattle\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     30\u001b[0m     perception_check_disadvantage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(perception_check, perception_check_2)\n",
      "File \u001b[0;32m~/workspace/natural_20.py/natural20/entity.py:69\u001b[0m, in \u001b[0;36mEntity.make_skill_check_function.<locals>.skill_check\u001b[0;34m(battle, **opts)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mskill_check\u001b[39m(battle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mopts):\n\u001b[0;32m---> 69\u001b[0m     modifiers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mskill\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_mod\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     description \u001b[38;5;241m=\u001b[39m opts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdice roll for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mskill\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DieRoll\u001b[38;5;241m.\u001b[39mroll_with_lucky(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1d20+\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodifiers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, description\u001b[38;5;241m=\u001b[39mdescription, battle\u001b[38;5;241m=\u001b[39mbattle)\n",
      "File \u001b[0;32m~/workspace/natural_20.py/natural20/entity.py:60\u001b[0m, in \u001b[0;36mEntity.make_skill_mod_function.<locals>.skill_mod\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproperties[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mskills\u001b[39m\u001b[38;5;124m'\u001b[39m][skill]\n\u001b[1;32m     59\u001b[0m modifiers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mability\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_mod\u001b[39m\u001b[38;5;124m\"\u001b[39m)()\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproficient_in\u001b[49m(skill):\n\u001b[1;32m     61\u001b[0m     bonus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproficiency_bonus() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_expertise_in(skill) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproficiency_bonus()\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PlayerCharacter' object has no attribute 'proficient_in'"
     ]
    }
   ],
   "source": [
    "env = make(\"dndenv-v0\", root_path=\"map_with_obstacles\", show_logs=True,\n",
    "           custom_session=session,\n",
    "           damaged_based_reward=True,\n",
    "           render_mode=\"ansi\")\n",
    "env.reset()\n",
    "print(env.render())\n",
    "print(env.observation_space)\n",
    "print(env.action_space.sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQN Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAJECTORY_POLICY = \"e-greedy\"\n",
    "NUM_UPDATES = 2\n",
    "TEMP_DECAY = 0.999\n",
    "BUFFER_CAPACITY = 2000\n",
    "FRAMES_TO_STORE = 2\n",
    "MAX_STEPS = 2000\n",
    "BATCH_SIZE = 64\n",
    "TARGET_UPDATE_FREQ = 1\n",
    "T_HORIZON = 2048\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_FINAL = 0.02\n",
    "EPSILON_DECAY_FRAMES = 10**3\n",
    "EVAL_STEPS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = QNetwork(device=device)\n",
    "model.to(device)\n",
    "state, info = env.reset()\n",
    "moves = info[\"available_moves\"]\n",
    "model.eval()\n",
    "print(model(state, moves[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def act_with_policy(state, info, model, policy='e-greedy', temperature=5.0, epsilon=0.1):\n",
    "    available_moves = info[\"available_moves\"]\n",
    "    with torch.no_grad():\n",
    "        if policy == 'boltzmann':\n",
    "            values = torch.stack([model(state, move).squeeze() for move in available_moves])\n",
    "            if len(values) > 1:\n",
    "                if temperature != 0:\n",
    "                    values = values / temperature\n",
    "                else:\n",
    "                    raise ValueError(\"Temperature is zero, which can lead to division by zero.\")\n",
    "\n",
    "                # Stabilizing the exponential calculation\n",
    "                values = values - torch.max(values)  # Subtract the max value for numerical stability\n",
    "                values = torch.exp(values)\n",
    "                sum_values = torch.sum(values)\n",
    "\n",
    "                if sum_values > 0:\n",
    "                    values = values / sum_values\n",
    "                    chosen_index = torch.multinomial(values, 1).item()\n",
    "                else:\n",
    "                    print(\"Sum of exponentiated values is zero. Adjust the model or input.\")\n",
    "                    chosen_index = torch.randint(len(available_moves), (1,)).item()\n",
    "            else:\n",
    "                chosen_index = 0\n",
    "        elif policy == 'e-greedy':\n",
    "            if random.random() < epsilon:\n",
    "                chosen_index = random.choice(range(len(available_moves)))\n",
    "            else:\n",
    "                values = torch.stack([model(state, move) for move in available_moves])\n",
    "                chosen_index = torch.argmax(values).item()\n",
    "        elif policy == 'greedy':\n",
    "                values = torch.stack([model(state, move) for move in available_moves])\n",
    "                chosen_index = torch.argmax(values).item()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown policy: {policy}\")\n",
    "    \n",
    "    return available_moves[chosen_index]\n",
    "\n",
    "def generate_trajectory(env, model, policy='e-greedy', temperature=5.0, epsilon=0.1, horizon=2048, quick_exit=False):\n",
    "    state, info = env.reset()\n",
    "    done = False\n",
    "    truncated = False\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    dones = []\n",
    "    truncateds = []\n",
    "    infos = []\n",
    "    truncated = False\n",
    "    for _ in range(horizon):\n",
    "        # instead of sampling  (e.g. env.action_space.sample()) we can ask help from the enivronment to obtain valid moves\n",
    "        # as there are sparse valid moves in the environment\n",
    "        action = act_with_policy(state, info, model, policy, temperature, epsilon)\n",
    "        next_state, reward, done, truncated, next_info = env.step(action)       \n",
    "        \n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "        truncateds.append(truncated)\n",
    "        infos.append(info)\n",
    "\n",
    "        if done:\n",
    "            break    \n",
    "        if truncated:\n",
    "            truncated = True\n",
    "            break\n",
    "        state = next_state\n",
    "        info = next_info\n",
    "        \n",
    "    states.append(next_state)\n",
    "    infos.append(next_info)\n",
    "    actions.append((-1, (0,0), (0,0), 0))\n",
    "    return states, actions, rewards, dones, truncateds, infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory = generate_trajectory(env, model, epsilon=1.0)\n",
    "print(trajectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 10\n",
    "total_rewards = 0\n",
    "for i in tqdm.tqdm(range(EPISODES)):\n",
    "    states, actions, rewards, dones, truncateds, infos = generate_trajectory(env, model, epsilon=1.0)\n",
    "    total_rewards += sum(rewards)\n",
    "\n",
    "avg_reward = total_rewards/EPISODES\n",
    "print(f\"Average reward: {avg_reward} Total Reward: {total_rewards}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, states, actions, rewards, infos, is_terminal):\n",
    "        self.buffer.append((states, actions, rewards, infos, is_terminal))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.choice(len(self.buffer), batch_size)\n",
    "        states, actions, rewards, infos, is_terminals = zip(*[self.buffer[idx] for idx in indices])\n",
    "        return states, actions, rewards, infos, is_terminals\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    # memory usage of the buffer in bytes\n",
    "    def memory_usage(self):\n",
    "        total_size = 0\n",
    "        for item in self.buffer:\n",
    "            states, actions, rewards, infos, is_terminals = item\n",
    "            for s in states:\n",
    "                total_size += sys.getsizeof(s)\n",
    "            total_size += sys.getsizeof(actions)\n",
    "            total_size += sys.getsizeof(rewards)\n",
    "            total_size += sys.getsizeof(infos)\n",
    "            total_size += sys.getsizeof(is_terminals)\n",
    "\n",
    "        return total_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a batch of trajectories and store them in the replay buffer\n",
    "def generate_batch_trajectories(env, model, n_rollout, replay_buffer: ReplayBuffer, temperature=5.0, epsilon=0.1, horizon=30, policy='e-greedy'):\n",
    "    # print(f\"generating {n_rollout} rollouts\")\n",
    "    for _ in range(n_rollout):\n",
    "        state, action, reward, done, truncated, info = generate_trajectory(env, model, temperature=temperature,\n",
    "                                                                           epsilon=epsilon,\n",
    "                                                                           horizon=horizon,policy=policy)\n",
    "        replay_buffer.push(state, action, reward, info, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, gamma, learning_rate, max_steps=MAX_STEPS, use_td_target=True,\n",
    "          trajectory_policy='e-greedy',\n",
    "          label=\"dnd_egreedy\",\n",
    "          n_rollout=8,\n",
    "          seed=1337):\n",
    "  print(f\"training with gamma {gamma} and learning rate {learning_rate}\")\n",
    "  env.seed(seed)\n",
    "\n",
    "  replay_buffer = ReplayBuffer(BUFFER_CAPACITY)\n",
    "  # load model checkpoint if available\n",
    "  model = QNetwork(device).to(device)\n",
    "  target_model = QNetwork(device).to(device)\n",
    "\n",
    "  # intialize target network with the same weights as the model\n",
    "  target_model.load_state_dict(model.state_dict())\n",
    "\n",
    "  optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "  best_avg = -10\n",
    "  best_step = 0\n",
    "  temperature = 5.0\n",
    "  reward_per_episode = []\n",
    "  epsilon = EPSILON_START\n",
    "\n",
    "  for step in tqdm.tqdm(range(max_steps)):\n",
    "    # if trajectory_policy == 'boltzmann':\n",
    "    #   print(f\"step {step} t={temperature}\")\n",
    "    # if trajectory_policy == 'softmax':\n",
    "    #   print(f\"step {step} t={temperature}\")\n",
    "    # elif trajectory_policy == 'e-greedy':\n",
    "    #   print(f\"step {step} epsilon={epsilon}\")\n",
    "    # else:\n",
    "    #   print(f\"step {step}\")\n",
    "    generate_batch_trajectories(env, model, n_rollout, replay_buffer, temperature=temperature,\n",
    "                                epsilon=epsilon, policy=trajectory_policy, horizon=T_HORIZON)\n",
    "\n",
    "    states, actions, rewards, infos, is_terminals = replay_buffer.sample(BATCH_SIZE)\n",
    "    rewards_collected = 0\n",
    "    for _ in range(NUM_UPDATES):\n",
    "      rewards_collected = 0\n",
    "      total_loss = 0.0\n",
    "      \n",
    "      for i in range(len(states)):\n",
    "        s = states[i]\n",
    "        a = actions[i]\n",
    "        env_info = infos[i]\n",
    "        r = torch.tensor(rewards[i]).to(device).unsqueeze(1)\n",
    "        is_terminal = torch.tensor(is_terminals[i]).float().to(device).unsqueeze(1)\n",
    "        \n",
    "        if use_td_target:\n",
    "          with torch.no_grad():\n",
    "            s_next = s[1:]\n",
    "            a_next = a[1:]\n",
    "            q_targets = target_model(s_next, a_next).detach()\n",
    "        else: # Q-learning target == \"slow\"\n",
    "          with torch.no_grad():\n",
    "            s_next = s[1:]\n",
    "            s_info = env_info[1:]\n",
    "            q_targets = torch.zeros(len(s_next)).to(device)\n",
    "            \n",
    "            for index in range(len(s_info)):\n",
    "              info = s_info[index]\n",
    "              state = s_next[index]\n",
    "              \n",
    "              if len(state) == 0:\n",
    "                q_targets[index] = 0\n",
    "                continue\n",
    "              \n",
    "              total_available_moves = len(info[\"available_moves\"])\n",
    "              states_t = [state] * total_available_moves\n",
    "              avail_actions = info[\"available_moves\"]\n",
    "              assert len(states_t) > 0, \"No available states\"\n",
    "              assert len(avail_actions) > 0, \"No available moves\"\n",
    "              \n",
    "              q_values = target_model(states_t, avail_actions).detach().squeeze(1)\n",
    "              if len(q_values) == 0:\n",
    "                q_targets[index] = 0\n",
    "              else:\n",
    "                q_targets[index] = torch.max(q_values).item()\n",
    "\n",
    "            q_targets = q_targets.unsqueeze(1)\n",
    "            assert q_targets.shape == r.shape, f\"q_targets shape {q_targets.shape} != r shape {r.shape}\"\n",
    "\n",
    "        targets = r + gamma * q_targets * (1 - is_terminal)\n",
    "        \n",
    "        s_input = s[0:-1]\n",
    "        a_input = a[0:-1]\n",
    "        output = model(s_input, a_input)\n",
    "        q_sa = output\n",
    "\n",
    "        value_loss = nn.MSELoss()(q_sa, targets)\n",
    "        optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        total_loss += value_loss.item()\n",
    "        rewards_collected += r.sum().item()\n",
    "        optimizer.step()\n",
    "\n",
    "    # save model checkpoint\n",
    "\n",
    "    if step % 10 == 0:\n",
    "      # torch.save(model.state_dict(), f\"model_{step}.pt\")\n",
    "      eval_rewards = []\n",
    "      for _ in range(EVAL_STEPS):\n",
    "        _, _, rewards, _, _, _ = generate_trajectory(env, model, policy='greedy')\n",
    "        total_reward = sum(rewards)\n",
    "        eval_rewards.append(total_reward)\n",
    "        \n",
    "      avg_rewards = np.mean(eval_rewards)\n",
    "      # print(f\"eval rewards: {avg_rewards}\")\n",
    "      reward_per_episode.append(avg_rewards)\n",
    "\n",
    "      # print(f\"total reward: {total_reward}\")\n",
    "      print(f\"{step}: avg rewards {avg_rewards} best avg {best_avg}@{best_step}\")\n",
    "\n",
    "      if avg_rewards > best_avg:\n",
    "        print(f\"best: {avg_rewards}\")\n",
    "        best_avg = avg_rewards\n",
    "        best_step = step\n",
    "        torch.save(model.state_dict(), f\"model_best_{label}.pt\")\n",
    "\n",
    "      torch.save(model.state_dict(), f\"model_{label}_{gamma}_{learning_rate}.pt\")\n",
    "\n",
    "\n",
    "    if step % 100 == 0:\n",
    "      torch.save(model.state_dict(), f\"model_{label}_{gamma}_{learning_rate}_{step}.pt\")\n",
    "\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "    # decay temp\n",
    "    temperature = np.max([0.1, temperature * TEMP_DECAY])\n",
    "\n",
    "    # decay epsilon\n",
    "    epsilon = EPSILON_FINAL + (EPSILON_START - EPSILON_FINAL) * np.exp(-1.0 * step / EPSILON_DECAY_FRAMES)\n",
    "\n",
    "    if step % TARGET_UPDATE_FREQ == 0:\n",
    "      # calculate the avg change weights of the model with the target model\n",
    "      total_change = 0\n",
    "      for p, p_target in zip(model.parameters(), target_model.parameters()):\n",
    "        total_change += torch.abs(p - p_target).sum().item()\n",
    "      # print(f\"total change: {total_change}\")\n",
    "\n",
    "      target_model.load_state_dict(model.state_dict())\n",
    "\n",
    "  env.close()\n",
    "  return reward_per_episode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the location of the game configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_setup_path = \"map_with_obstacles\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the env setup. Note that we use damaged based rewards to give a denser reward signalling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(root_path):\n",
    "    return make(\"dndenv-v0\", root_path=root_path, show_logs=False, damage_based_reward=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(game_setup_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1337\n",
    "# Create a grid of learning rates and gammas\n",
    "learning_rates = [0.0001]\n",
    "gammas = [0.99]\n",
    "\n",
    "results = {}\n",
    "for lr in learning_rates:\n",
    "  results[lr] = {}\n",
    "  for gamma in gammas:\n",
    "    seed = seed + 1\n",
    "    reward_per_episode = train(env, gamma, lr, max_steps=MAX_STEPS, seed=seed, use_td_target=False)\n",
    "    results[lr][gamma] = reward_per_episode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize rewards per episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in results:\n",
    "  for gamma in results[item]:\n",
    "    print(f\"lr: {item} gamma: {gamma} rewards: {results[item][gamma]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for gamma in gammas:\n",
    "        plt.plot(results[lr][gamma], label=f\"lr={lr}, gamma={gamma}\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(game_setup_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1337\n",
    "# Create a grid of learning rates and gammas\n",
    "learning_rates = [0.001]\n",
    "gammas = [0.99]\n",
    "\n",
    "results = {}\n",
    "for lr in learning_rates:\n",
    "  results[lr] = {}\n",
    "  for gamma in gammas:\n",
    "    seed = seed + 1\n",
    "    reward_per_episode = train(env, gamma, lr, max_steps=MAX_STEPS, seed=seed, trajectory_policy='boltzmann', label=\"boltzmann\")\n",
    "    results[lr][gamma] = reward_per_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in results:\n",
    "  for gamma in results[item]:\n",
    "    print(f\"lr: {item} gamma: {gamma} rewards: {results[item][gamma]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for gamma in gammas:\n",
    "        plt.plot(results[lr][gamma], label=f\"lr={lr}, gamma={gamma}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform some tests on the trained agent. Show a combat log from a fight against the rules based AI. Define a policy based on the Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "MAX_EPISODES = 500\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class ModelPolicy:\n",
    "    def __init__(self):\n",
    "        self.model = QNetwork(device=device)\n",
    "        self.model.to(device)\n",
    "        if not os.path.exists('model_best_blotzmann.pt'):\n",
    "            raise FileNotFoundError(\"Model file not found. Please run dnd_dqn.ipynb notebook to train an agent.\")\n",
    "        self.model.load_state_dict(torch.load('model_best_blotzmann.pt'))\n",
    "\n",
    "    def action(self, state, info):\n",
    "        available_moves = info[\"available_moves\"]\n",
    "        values = torch.stack([self.model(state, move) for move in available_moves])\n",
    "        for index, v in enumerate(values):\n",
    "            print(f\"{index}: {available_moves[index]} {v.item()}\")\n",
    "\n",
    "        chosen_index = torch.argmax(values).item()\n",
    "        return available_moves[chosen_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = make(\"dndenv-v0\", root_path=\"map_with_obstacles\", render_mode=\"ansi\", show_logs=True)\n",
    "\n",
    "observation, info = env.reset()\n",
    "\n",
    "print(\"=========================================\")\n",
    "print(\"Battle between an RL agent vs a Rules based AI\")\n",
    "print(\"=========================================\")\n",
    "model = ModelPolicy()\n",
    "action = action = model.action(observation, info)\n",
    "\n",
    "print(f\"selected action: {action}\")\n",
    "terminal = False\n",
    "episode = 0\n",
    "\n",
    "while not terminal and episode < MAX_EPISODES:\n",
    "    episode += 1\n",
    "    observation, reward, terminal, truncated, info = env.step(action)\n",
    "    print(env.render())\n",
    "    if not terminal and not truncated:\n",
    "        episode_name_with_padding = str(episode).zfill(3)\n",
    "\n",
    "        # display entity healths\n",
    "        print(f\"Turn {info['current_index']}\\n\")\n",
    "        print(f\"Reward: {reward}\\n\")\n",
    "        print(f\"health hero: {observation['health_pct']}\\n\")\n",
    "        print(f\"health enemy: {observation['health_enemy']}\\n\")\n",
    "        print(env.render())\n",
    "        \n",
    "        action = model.action(observation, info)\n",
    "        print(f\"agent selected action: {action}\")\n",
    "\n",
    "    if terminal or truncated:\n",
    "        print(f\"Reward: {reward}\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
