{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyyaml in /home/jedld/.local/lib/python3.10/site-packages (6.0.1)\n",
      "Requirement already satisfied: dndice in /home/jedld/.local/lib/python3.10/site-packages (2.8.1)\n",
      "Requirement already satisfied: python-i18n in /home/jedld/.local/lib/python3.10/site-packages (0.3.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyyaml\n",
    "!pip install dndice\n",
    "!pip install python-i18n\n",
    "!pip install gymnasium\n",
    "!pip install inflect\n",
    "!pip install collections-extended\n",
    "!pip install openai\n",
    "!pip install -e ..\n",
    "!pip install ipywidgets\n",
    "!pip install iprogress\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium import make\n",
    "from model import QNetwork\n",
    "from natural20.gym.dndenv import dndenv\n",
    "import torch\n",
    "import tqdm as tqdm\n",
    "import tqdm.notebook as tqdm\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import gc\n",
    "import numpy as np\n",
    "import sys\n",
    "import collections\n",
    "from natural20.utils.utils import Session\n",
    "from natural20.event_manager import EventManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "  device = torch.device(\"mps\")\n",
    "else:\n",
    "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config = \"map_with_obstacles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = Session(env_config, event_manager=EventManager())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show info about the environment and a render of the tabletop map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make(\"dndenv-v0\", root_path=\"map_with_obstacles\", show_logs=True,\n",
    "           custom_session=session,\n",
    "           damaged_based_reward=True,\n",
    "           render_mode=\"ansi\")\n",
    "env.reset()\n",
    "print(env.render())\n",
    "print(env.observation_space)\n",
    "print(env.action_space.sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQN Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAJECTORY_POLICY = \"e-greedy\"\n",
    "NUM_UPDATES = 2\n",
    "TEMP_DECAY = 0.999\n",
    "BUFFER_CAPACITY = 2000\n",
    "FRAMES_TO_STORE = 2\n",
    "MAX_STEPS = 3000\n",
    "BATCH_SIZE = 64\n",
    "TARGET_UPDATE_FREQ = 1\n",
    "T_HORIZON = 2048\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_FINAL = 0.02\n",
    "EPSILON_DECAY_FRAMES = 10**3\n",
    "EVAL_STEPS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = QNetwork(device=device)\n",
    "model.to(device)\n",
    "state, info = env.reset()\n",
    "moves = info[\"available_moves\"]\n",
    "model.eval()\n",
    "print(model(state, moves[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def act_with_policy(state, info, model, policy='e-greedy', temperature=5.0, epsilon=0.1):\n",
    "    available_moves = info[\"available_moves\"]\n",
    "    with torch.no_grad():\n",
    "        if policy == 'boltzmann':\n",
    "            values = torch.stack([model(state, move).squeeze() for move in available_moves])\n",
    "            if len(values) > 1:\n",
    "                if temperature != 0:\n",
    "                    values = values / temperature\n",
    "                else:\n",
    "                    raise ValueError(\"Temperature is zero, which can lead to division by zero.\")\n",
    "\n",
    "                # Stabilizing the exponential calculation\n",
    "                values = values - torch.max(values)  # Subtract the max value for numerical stability\n",
    "                values = torch.exp(values)\n",
    "                sum_values = torch.sum(values)\n",
    "\n",
    "                if sum_values > 0:\n",
    "                    values = values / sum_values\n",
    "                    chosen_index = torch.multinomial(values, 1).item()\n",
    "                else:\n",
    "                    print(\"Sum of exponentiated values is zero. Adjust the model or input.\")\n",
    "                    chosen_index = torch.randint(len(available_moves), (1,)).item()\n",
    "            else:\n",
    "                chosen_index = 0\n",
    "        elif policy == 'e-greedy':\n",
    "            if random.random() < epsilon:\n",
    "                chosen_index = random.choice(range(len(available_moves)))\n",
    "            else:\n",
    "                values = torch.stack([model(state, move) for move in available_moves])\n",
    "                chosen_index = torch.argmax(values).item()\n",
    "        elif policy == 'greedy':\n",
    "                values = torch.stack([model(state, move) for move in available_moves])\n",
    "                chosen_index = torch.argmax(values).item()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown policy: {policy}\")\n",
    "    \n",
    "    return available_moves[chosen_index]\n",
    "\n",
    "def generate_trajectory(env, model, policy='e-greedy', temperature=5.0, epsilon=0.1, horizon=2048, quick_exit=False):\n",
    "    state, info = env.reset()\n",
    "    done = False\n",
    "    truncated = False\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    dones = []\n",
    "    truncateds = []\n",
    "    infos = []\n",
    "    truncated = False\n",
    "    for _ in range(horizon):\n",
    "        # instead of sampling  (e.g. env.action_space.sample()) we can ask help from the enivronment to obtain valid moves\n",
    "        # as there are sparse valid moves in the environment\n",
    "        action = act_with_policy(state, info, model, policy, temperature, epsilon)\n",
    "        next_state, reward, done, truncated, next_info = env.step(action)       \n",
    "        \n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "        truncateds.append(truncated)\n",
    "        infos.append(info)\n",
    "\n",
    "        if done:\n",
    "            break    \n",
    "        if truncated:\n",
    "            truncated = True\n",
    "            break\n",
    "        state = next_state\n",
    "        info = next_info\n",
    "        \n",
    "    states.append(next_state)\n",
    "    infos.append(next_info)\n",
    "    actions.append((-1, (0,0), (0,0), 0, 0))\n",
    "    return states, actions, rewards, dones, truncateds, infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory = generate_trajectory(env, model, epsilon=1.0)\n",
    "print(trajectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 10\n",
    "total_rewards = 0\n",
    "for i in tqdm.tqdm(range(EPISODES)):\n",
    "    states, actions, rewards, dones, truncateds, infos = generate_trajectory(env, model, epsilon=1.0)\n",
    "    total_rewards += sum(rewards)\n",
    "\n",
    "avg_reward = total_rewards/EPISODES\n",
    "print(f\"Average reward: {avg_reward} Total Reward: {total_rewards}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, states, actions, rewards, infos, is_terminal):\n",
    "        self.buffer.append((states, actions, rewards, infos, is_terminal))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.choice(len(self.buffer), batch_size)\n",
    "        states, actions, rewards, infos, is_terminals = zip(*[self.buffer[idx] for idx in indices])\n",
    "        return states, actions, rewards, infos, is_terminals\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    # memory usage of the buffer in bytes\n",
    "    def memory_usage(self):\n",
    "        total_size = 0\n",
    "        for item in self.buffer:\n",
    "            states, actions, rewards, infos, is_terminals = item\n",
    "            for s in states:\n",
    "                total_size += sys.getsizeof(s)\n",
    "            total_size += sys.getsizeof(actions)\n",
    "            total_size += sys.getsizeof(rewards)\n",
    "            total_size += sys.getsizeof(infos)\n",
    "            total_size += sys.getsizeof(is_terminals)\n",
    "\n",
    "        return total_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a batch of trajectories and store them in the replay buffer\n",
    "def generate_batch_trajectories(env, model, n_rollout, replay_buffer: ReplayBuffer, temperature=5.0, epsilon=0.1, horizon=30, policy='e-greedy'):\n",
    "    # print(f\"generating {n_rollout} rollouts\")\n",
    "    for _ in range(n_rollout):\n",
    "        state, action, reward, done, truncated, info = generate_trajectory(env, model, temperature=temperature,\n",
    "                                                                           epsilon=epsilon,\n",
    "                                                                           horizon=horizon,policy=policy)\n",
    "        replay_buffer.push(state, action, reward, info, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, gamma, learning_rate, max_steps=MAX_STEPS, use_td_target=True,\n",
    "          trajectory_policy='e-greedy',\n",
    "          label=\"dnd_egreedy\",\n",
    "          n_rollout=8,\n",
    "          seed=1337):\n",
    "  print(f\"training with gamma {gamma} and learning rate {learning_rate}\")\n",
    "  env.seed(seed)\n",
    "\n",
    "  replay_buffer = ReplayBuffer(BUFFER_CAPACITY)\n",
    "  # load model checkpoint if available\n",
    "  model = QNetwork(device).to(device)\n",
    "  target_model = QNetwork(device).to(device)\n",
    "\n",
    "  # intialize target network with the same weights as the model\n",
    "  target_model.load_state_dict(model.state_dict())\n",
    "\n",
    "  optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "  best_avg = -10\n",
    "  best_step = 0\n",
    "  temperature = 5.0\n",
    "  reward_per_episode = []\n",
    "  epsilon = EPSILON_START\n",
    "\n",
    "  for step in tqdm.tqdm(range(max_steps)):\n",
    "    generate_batch_trajectories(env, model, n_rollout, replay_buffer, temperature=temperature,\n",
    "                                epsilon=epsilon, policy=trajectory_policy, horizon=T_HORIZON)\n",
    "\n",
    "    states, actions, rewards, infos, is_terminals = replay_buffer.sample(BATCH_SIZE)\n",
    "    rewards_collected = 0\n",
    "    for _ in range(NUM_UPDATES):\n",
    "      rewards_collected = 0\n",
    "      total_loss = 0.0\n",
    "      \n",
    "      for i in range(len(states)):\n",
    "        s = states[i]\n",
    "        a = actions[i]\n",
    "        env_info = infos[i]\n",
    "        r = torch.tensor(rewards[i]).to(device).unsqueeze(1)\n",
    "        is_terminal = torch.tensor(is_terminals[i]).float().to(device).unsqueeze(1)\n",
    "        \n",
    "        if use_td_target:\n",
    "          with torch.no_grad():\n",
    "            s_next = s[1:]\n",
    "            a_next = a[1:]\n",
    "            q_targets = target_model(s_next, a_next).detach()\n",
    "        else: # Q-learning target == \"slow\"\n",
    "          with torch.no_grad():\n",
    "            s_next = s[1:]\n",
    "            s_info = env_info[1:]\n",
    "            q_targets = torch.zeros(len(s_next)).to(device)\n",
    "            \n",
    "            for index in range(len(s_info)):\n",
    "              info = s_info[index]\n",
    "              state = s_next[index]\n",
    "              \n",
    "              if len(state) == 0:\n",
    "                q_targets[index] = 0\n",
    "                continue\n",
    "              \n",
    "              total_available_moves = len(info[\"available_moves\"])\n",
    "              states_t = [state] * total_available_moves\n",
    "              avail_actions = info[\"available_moves\"]\n",
    "              assert len(states_t) > 0, \"No available states\"\n",
    "              assert len(avail_actions) > 0, \"No available moves\"\n",
    "              \n",
    "              q_values = target_model(states_t, avail_actions).detach().squeeze(1)\n",
    "              if len(q_values) == 0:\n",
    "                q_targets[index] = 0\n",
    "              else:\n",
    "                q_targets[index] = torch.max(q_values).item()\n",
    "\n",
    "            q_targets = q_targets.unsqueeze(1)\n",
    "            assert q_targets.shape == r.shape, f\"q_targets shape {q_targets.shape} != r shape {r.shape}\"\n",
    "\n",
    "        targets = r + gamma * q_targets * (1 - is_terminal)\n",
    "        \n",
    "        s_input = s[0:-1]\n",
    "        a_input = a[0:-1]\n",
    "        output = model(s_input, a_input)\n",
    "        q_sa = output\n",
    "\n",
    "        value_loss = nn.MSELoss()(q_sa, targets)\n",
    "        optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        total_loss += value_loss.item()\n",
    "        rewards_collected += r.sum().item()\n",
    "        optimizer.step()\n",
    "\n",
    "    # save model checkpoint\n",
    "\n",
    "    if step % 10 == 0:\n",
    "      # torch.save(model.state_dict(), f\"model_{step}.pt\")\n",
    "      eval_rewards = []\n",
    "      for _ in range(EVAL_STEPS):\n",
    "        _, _, rewards, _, _, _ = generate_trajectory(env, model, policy='greedy')\n",
    "        total_reward = sum(rewards)\n",
    "        eval_rewards.append(total_reward)\n",
    "        \n",
    "      avg_rewards = np.mean(eval_rewards)\n",
    "      # print(f\"eval rewards: {avg_rewards}\")\n",
    "      reward_per_episode.append(avg_rewards)\n",
    "\n",
    "      # print(f\"total reward: {total_reward}\")\n",
    "      print(f\"{step}: avg rewards {avg_rewards} best avg {best_avg}@{best_step}\")\n",
    "\n",
    "      if avg_rewards > best_avg:\n",
    "        print(f\"best: {avg_rewards}\")\n",
    "        best_avg = avg_rewards\n",
    "        best_step = step\n",
    "        torch.save(model.state_dict(), f\"model_best_{label}.pt\")\n",
    "\n",
    "      # torch.save(model.state_dict(), f\"model_{label}_{gamma}_{learning_rate}.pt\")\n",
    "\n",
    "\n",
    "    if step % 100 == 0:\n",
    "      torch.save(model.state_dict(), f\"model_{label}_{gamma}_{learning_rate}_{step}.pt\")\n",
    "\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "    # decay temp\n",
    "    temperature = np.max([0.1, temperature * TEMP_DECAY])\n",
    "\n",
    "    # decay epsilon\n",
    "    epsilon = EPSILON_FINAL + (EPSILON_START - EPSILON_FINAL) * np.exp(-1.0 * step / EPSILON_DECAY_FRAMES)\n",
    "\n",
    "    if step % TARGET_UPDATE_FREQ == 0:\n",
    "      # calculate the avg change weights of the model with the target model\n",
    "      total_change = 0\n",
    "      for p, p_target in zip(model.parameters(), target_model.parameters()):\n",
    "        total_change += torch.abs(p - p_target).sum().item()\n",
    "      # print(f\"total change: {total_change}\")\n",
    "\n",
    "      target_model.load_state_dict(model.state_dict())\n",
    "\n",
    "  env.close()\n",
    "  return reward_per_episode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the location of the game configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_setup_path = \"map_with_obstacles\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the env setup. Note that we use damaged based rewards to give a denser reward signalling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(root_path, render_mode=\"ansi\", show_logs=False):\n",
    "    return make(\"dndenv-v0\", root_path=root_path, show_logs=show_logs,\n",
    "                render_mode=render_mode,\n",
    "                damage_based_reward=False,\n",
    "                profiles=['high_elf_mage'],\n",
    "                enemies=['halfling_rogue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(game_setup_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1337\n",
    "# Create a grid of learning rates and gammas\n",
    "learning_rates = [0.0001]\n",
    "gammas = [0.99]\n",
    "\n",
    "results = {}\n",
    "for lr in learning_rates:\n",
    "  results[lr] = {}\n",
    "  for gamma in gammas:\n",
    "    seed = seed + 1\n",
    "    reward_per_episode = train(env, gamma, lr, max_steps=MAX_STEPS, seed=seed, use_td_target=False)\n",
    "    results[lr][gamma] = reward_per_episode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize rewards per episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in results:\n",
    "  for gamma in results[item]:\n",
    "    print(f\"lr: {item} gamma: {gamma} rewards: {results[item][gamma]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for gamma in gammas:\n",
    "        plt.plot(results[lr][gamma], label=f\"lr={lr}, gamma={gamma}\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(game_setup_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1337\n",
    "# Create a grid of learning rates and gammas\n",
    "learning_rates = [0.001]\n",
    "gammas = [0.99]\n",
    "\n",
    "results = {}\n",
    "for lr in learning_rates:\n",
    "  results[lr] = {}\n",
    "  for gamma in gammas:\n",
    "    seed = seed + 1\n",
    "    reward_per_episode = train(env, gamma, lr, max_steps=MAX_STEPS, seed=seed, trajectory_policy='boltzmann', label=\"boltzmann\")\n",
    "    results[lr][gamma] = reward_per_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in results:\n",
    "  for gamma in results[item]:\n",
    "    print(f\"lr: {item} gamma: {gamma} rewards: {results[item][gamma]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for gamma in gammas:\n",
    "        plt.plot(results[lr][gamma], label=f\"lr={lr}, gamma={gamma}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform some tests on the trained agent. Show a combat log from a fight against the rules based AI. Define a policy based on the Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "MAX_EPISODES = 500\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class ModelPolicy:\n",
    "    def __init__(self, weights_file = 'model_best_boltzmann.pt'):\n",
    "        self.model = QNetwork(device=device)\n",
    "        self.model.to(device)\n",
    "        if not os.path.exists(weights_file):\n",
    "            raise FileNotFoundError(f\"Model file {weights_file} not found. Please run dnd_dqn.ipynb notebook to train an agent.\")\n",
    "        self.model.load_state_dict(torch.load(weights_file))\n",
    "\n",
    "    def action(self, state, info):\n",
    "        available_moves = info[\"available_moves\"]\n",
    "        values = torch.stack([self.model(state, move) for move in available_moves])\n",
    "        for index, v in enumerate(values):\n",
    "            print(f\"{index}: {available_moves[index]} {v.item()}\")\n",
    "\n",
    "        chosen_index = torch.argmax(values).item()\n",
    "        return available_moves[chosen_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = make_env(game_setup_path)\n",
    "\n",
    "observation, info = env.reset()\n",
    "\n",
    "print(\"=========================================\")\n",
    "print(\"Battle between an RL agent vs a Rules based AI\")\n",
    "print(\"=========================================\")\n",
    "model = ModelPolicy()\n",
    "action = action = model.action(observation, info)\n",
    "\n",
    "print(f\"selected action: {action}\")\n",
    "terminal = False\n",
    "episode = 0\n",
    "\n",
    "while not terminal and episode < MAX_EPISODES:\n",
    "    episode += 1\n",
    "    observation, reward, terminal, truncated, info = env.step(action)\n",
    "    print(env.render())\n",
    "    if not terminal and not truncated:\n",
    "        episode_name_with_padding = str(episode).zfill(3)\n",
    "\n",
    "        # display entity healths\n",
    "        print(f\"Turn {info['current_index']}\\n\")\n",
    "        print(f\"Reward: {reward}\\n\")\n",
    "        print(f\"health hero: {observation['health_pct']}\\n\")\n",
    "        print(f\"health enemy: {observation['health_enemy']}\\n\")\n",
    "        print(env.render())\n",
    "        \n",
    "        action = model.action(observation, info)\n",
    "        print(f\"agent selected action: {action}\")\n",
    "\n",
    "    if terminal or truncated:\n",
    "        print(f\"Reward: {reward}\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
